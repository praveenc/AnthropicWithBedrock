{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6834f0e3-00f7-4e5d-9627-bf4bd08e50c8",
   "metadata": {},
   "source": [
    "# Adding Memory to Conversations\n",
    "\n",
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships.\n",
    "\n",
    "We call this ability to store information about past interactions \"memory\". LangChain provides a lot of utilities for adding memory to a system. These utilities can be used by themselves or incorporated seamlessly into a chain.\n",
    "\n",
    "A memory system needs to support two basic actions: reading and writing. Recall that every chain defines some core execution logic that expects certain inputs. Some of these inputs come directly from the user, but some of these inputs can come from memory. A chain will interact with its memory system twice in a given run.\n",
    "\n",
    "1. `AFTER` receiving the initial user inputs but `BEFORE` executing the core logic, a chain will READ from its memory system and augment the user inputs.\n",
    "2. `AFTER` executing the core logic but `BEFORE` returning the answer, a chain will WRITE the inputs and outputs of the current run to memory, so that they can be referred to in future runs.\n",
    "\n",
    "Reference: <https://python.langchain.com/docs/modules/memory/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4662ddc-0307-407c-916c-538ed509eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from utils import get_inference_parameters\n",
    "from rich import print\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6bafa5f-e79b-420b-a0c1-fedcd44d7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\", region_name=\"us-west-2\"\n",
    ")  # bedrock runtime boto3 client\n",
    "bedrock_model_id = \"anthropic.claude-v2\"  # Amazon Bedrock Model ID for claude-v2\n",
    "\n",
    "model_kwargs = get_inference_parameters(\"anthropic\")  # anthropic claude model parameters\n",
    "\n",
    "# Define the llm with langchain.llms.bedrock\n",
    "model = Bedrock(\n",
    "    model_id=bedrock_model_id, client=bedrock_runtime, model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f561b-8043-4ade-aaae-77c72e4f8acc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Conversation Buffer Memory\n",
    "\n",
    "A memory system needs to support two basic actions: reading and writing. Recall that every chain defines some core execution logic that expects certain inputs. Some of these inputs come directly from the user, but some of these inputs can come from memory. A chain will interact with its memory system twice in a given run.\n",
    "\n",
    "1. AFTER receiving the initial user inputs but BEFORE executing the core logic, a chain will READ from its memory system and augment the user inputs.\n",
    "2. AFTER executing the core logic but BEFORE returning the answer, a chain will WRITE the inputs and outputs of the current run to memory, so that they can be referred to in future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f956f1-31a0-4626-a372-133c64cf12b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", \"You are a helpful chatbot\"),\n",
    "#         MessagesPlaceholder(variable_name=\"history\"),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# memory = ConversationBufferMemory(return_messages=True, ai_prefix=\"assistant\")\n",
    "# memory.chat_memory.add_user_message(\"howdy claude!\")\n",
    "# memory.chat_memory.add_ai_message(\"Hi there! How are you?\")\n",
    "# memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "# Notice that \"chat_history\" is present in the prompt template\n",
    "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "Response:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a55c9176-7344-4b7c-88a7-64a9e1f9ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables)\n",
    "        | itemgetter(\"chat_history\")\n",
    "    )\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "inputs = {\"question\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response  # str\n",
    "memory.save_context(inputs, {\"output\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87737310-d4c2-4726-b9d5-ebd4c346603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi im bob'),\n",
       "  AIMessage(content=\" Hello Bob, nice to meet you! I'm Claude, an AI assistant created by Anthropic.\")]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab008d95-7e8b-4f03-a9c7-13a904fd839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"how did I introduce myself?\"}\n",
    "response = chain.invoke(inputs)\n",
    "response\n",
    "memory.save_context(inputs, {\"output\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e43de11a-4957-4dc9-84a0-f50978fc66a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi im bob'),\n",
       "  AIMessage(content=\" Hello Bob, nice to meet you! I'm Claude, an AI assistant created by Anthropic.\"),\n",
       "  HumanMessage(content='how did I introduce myself?'),\n",
       "  AIMessage(content=' Based on the previous conversation, it seems you introduced yourself by saying \"hi im bob\".')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471e304-6e9c-4a79-bc0c-32b845a6ca40",
   "metadata": {},
   "source": [
    "### Conversation Buffer Window\n",
    "\n",
    "ConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions.\n",
    "\n",
    "This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5fe962e-349b-46f5-9b5a-ff0ec86c2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    "bw_memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "bw_memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
    "bw_memory.save_context(\n",
    "    {\"input\": \"is it going well\"},\n",
    "    {\"output\": \"yes, it is going well for me. Thanks for asking\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9afa712-fd3b-4481-86df-8081d7b55482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content='whats up'),\n",
       "  HumanMessage(content='not much you'),\n",
       "  AIMessage(content='not much'),\n",
       "  HumanMessage(content='is it going well'),\n",
       "  AIMessage(content='yes, it is going well for me. Thanks for asking')]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44a56897-42e9-4192-8248-5ae7af41d12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='hi'), AIMessage(content='whats up'), HumanMessage(content='not much you'), AIMessage(content='not much'), HumanMessage(content='is it going well'), AIMessage(content='yes, it is going well for me. Thanks for asking')]\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Not much, just chatting with you! How's your day going?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=model,\n",
    "    # We set a low k=2, to only keep the last 2 interactions in memory\n",
    "    memory=bw_memory,\n",
    "    verbose=True,\n",
    ")\n",
    "conversation_with_summary.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06df5fe7-7694-4f6f-9035-adcea20305db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='not much you'), AIMessage(content='not much'), HumanMessage(content='is it going well'), AIMessage(content='yes, it is going well for me. Thanks for asking'), HumanMessage(content=\"Hi, what's up?\"), AIMessage(content=\" Not much, just chatting with you! How's your day going?\")]\n",
      "Human: What's their issues?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I\\'m afraid I don\\'t have enough context to know specifically who you are referring to with \"their\" or what \"issues\" you are asking about. As an AI assistant created by Anthropic to be helpful, harmless, and honest, I don\\'t actually have personal experiences or private knowledge about other people or their issues. Could you please rephrase your question or provide more details so I can better understand what you are asking? I\\'m happy to continue our conversation if you can clarify the context further.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"What's their issues?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e79a479-b2af-4a5d-ab67-c81198a1dcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='is it going well'), AIMessage(content='yes, it is going well for me. Thanks for asking'), HumanMessage(content=\"Hi, what's up?\"), AIMessage(content=\" Not much, just chatting with you! How's your day going?\"), HumanMessage(content=\"What's their issues?\"), AIMessage(content=' I\\'m afraid I don\\'t have enough context to know specifically who you are referring to with \"their\" or what \"issues\" you are asking about. As an AI assistant created by Anthropic to be helpful, harmless, and honest, I don\\'t actually have personal experiences or private knowledge about other people or their issues. Could you please rephrase your question or provide more details so I can better understand what you are asking? I\\'m happy to continue our conversation if you can clarify the context further.')]\n",
      "Human: Is it going well?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I don't have enough context to determine if something specific is going well. As an AI system, I don't have personal experiences or a notion of how things are going in general. I'm happy to continue our conversation if you can provide more details about what you are asking!\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Is it going well?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "nlp311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
