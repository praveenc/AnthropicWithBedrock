{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5b27ac-4417-4239-9e19-539aaf347463",
   "metadata": {},
   "source": [
    "# Anthropic function calling with Langchain Agents\n",
    "\n",
    "In this notebook we leveraage Langchain agents to perform function calling tasks. \n",
    "\n",
    "First we install the necessary python packages such as `duckduckgo-search`, `wikipedia`, and `xmltodict`, amongst others. \n",
    "\n",
    "- Initialize connection to **anthropic.claude-v2** using Bedrock\n",
    "- Define and test the following tools individually\n",
    "  - Wikipedia\n",
    "  - Web Search and\n",
    "  - PubMed search\n",
    "- Finally, we put this all together by initializing a Langchain agent executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d934971-c3ab-4aff-9c2b-79b5e2a46bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U duckduckgo-search wikipedia unstructured xmltodict --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25ff7655-f260-4057-9c9f-fd251f2a2f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rich extension is already loaded. To reload it, use:\n",
      "  %reload_ext rich\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "from anthropic_bedrock import AI_PROMPT, HUMAN_PROMPT, AnthropicBedrock\n",
    "from IPython.display import Markdown\n",
    "from langchain.agents import AgentType, Tool, initialize_agent, load_tools\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.tools import DuckDuckGoSearchRun, PubmedQueryRun, tool\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from rich import print\n",
    "from utils import get_inference_parameters\n",
    "\n",
    "%load_ext rich\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b078e7fd-fab7-4385-b459-3fe33ebfb769",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-west-2\"\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\", region_name=region\n",
    ")  # bedrock runtime boto3 client\n",
    "bedrock_model_id = \"anthropic.claude-v2\"  # Amazon Bedrock Model ID for claude-v2\n",
    "\n",
    "# anthropic claude model parameters\n",
    "model_kwargs = get_inference_parameters(\"anthropic\")\n",
    "\n",
    "# print(model_kwargs)\n",
    "\n",
    "# Define the llm with langchain.llms.bedrock\n",
    "bedrock_llm = Bedrock(\n",
    "    model_id=bedrock_model_id, client=bedrock_runtime, model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7cd42-a642-4d67-8383-397aa3cadf4e",
   "metadata": {},
   "source": [
    "## Let's define a few tools\n",
    "\n",
    "- [Wikipedia](https://wikipedia.org/) (Search wikipedia pages)\n",
    "- Web Search (Search web using [DuckDuckGo](https://duckduckgo.com/) search engine)\n",
    "- [PubMed](https://pubmed.ncbi.nlm.nih.gov/) (Medical papers and articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891751e3-9edc-4308-a749-6a1f3a2dcc8f",
   "metadata": {},
   "source": [
    "#### 01.Wikipedia Tool\n",
    "\n",
    "[Wikipedia](https://wikipedia.org/) is a multilingual free online encyclopedia written and maintained by a community of volunteers, through open collaboration. Wikipedia is the largest and most-read reference work in history. \n",
    "\n",
    ">*NOTE:* To use this tool, install `wikipedia` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948adab2-65ba-48c6-948a-749357f7059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API wrapper for Wikipedia\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "\n",
    "# Define the Wikipedia tool with a description and the function to retrieve results from Wikipedia\n",
    "wikipedia_tool = Tool(\n",
    "    name=\"wikipedia\",\n",
    "    func=wikipedia.run,\n",
    "    description=\"Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\",\n",
    ")\n",
    "\n",
    "# Test the defined Wikipedia tool\n",
    "# Markdown(wikipedia_tool(\"What is the Archimedes Principle?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee94af3-3863-494e-86be-c9466aeeffb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 02.DuckDuckGo Search Tool\n",
    "\n",
    "[DuckDuckGo](https://duckduckgo.com/) is an internet privacy company most popularly known for their private search engine. The company emphasizes privacy and anonimity as one of the key principles behind all their products.\n",
    "\n",
    ">*NOTE:* To use this tool, install `duckduckgo-search` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c4ab023-0fdc-4f68-bd02-6548e62c4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API wrapper for DuckDuckGo search\n",
    "duckduckgo_search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Define the DuckDuckGo tool using a description and the function to retrieve results from DuckDuckGo search\n",
    "duckduckgo_tool = Tool(\n",
    "    name=\"DuckDuckGoSearch\",\n",
    "    func=duckduckgo_search.run,\n",
    "    description=\"useful for when you need to answer questions about current events\",\n",
    ")\n",
    "\n",
    "# Test the DuckDuckGo tool\n",
    "# Markdown(duckduckgo_tool(\"Weather in Seattle, Washington?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1afa8-0148-40e3-b520-48f4895c9988",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 03.PubMed Tool\n",
    "\n",
    "[PubMed](https://pubmed.ncbi.nlm.nih.gov/) is a free search engine for medical papers and articles.\n",
    "\n",
    ">*NOTE:* To use this tool, install `xmltodict` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8b874f-ed91-43b3-b539-0a3182cc0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API wrapper for PubMed search\n",
    "pubmed_search = PubmedQueryRun()\n",
    "\n",
    "# Define the PubMed tool using a description and the function to retrieve results from PubMed\n",
    "pubmed_tool = Tool(\n",
    "    name=\"PubmedQueryRun\",\n",
    "    func=pubmed_search.run,\n",
    "    description=\"Useful for when you need medical information\",\n",
    ")\n",
    "\n",
    "# Test PubMed tool\n",
    "# Markdown(pubmed_tool(\"Covid diagnosis\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18036a-665e-4b98-860c-667becfc3e8b",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "- Define all the tools as a list\n",
    "- Initialize the agent with tool list and `AgentType` of type `ZERO_SHOT_REACT_DESCRIPTION`\n",
    "- Explicity tell the agent to handle parsing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0a2fff6-95d7-47db-b385-40bcd7329715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Here are the key points about SageMaker serverless inference endpoints:\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Amazon SageMaker\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Amazon SageMaker\n",
      "Summary: Amazon SageMaker is a cloud based machine-learning platform that allows the creation, training, and deployment by developers of machine-learning (ML) models on the cloud. It can be used to deploy ML models on embedded systems and edge-devices. SageMaker was launched in November 2017.\n",
      "\n",
      "Page: Amazon Polly\n",
      "Summary: Amazon Polly is a cloud service by Amazon Web Services, a subsidiary of Amazon.com, that converts text into spoken audio. It allows developers to create speech-enabled applications and products. It was launched in November 2016 and now includes 60 voices across 29 languages, some of which are Neural Text-to-Speech voices of higher quality. Users include Duolingo, a language education platform.\n",
      "\n",
      "Page: GPT-J\n",
      "Summary: GPT-J or GPT-J-6B is an open-source large language model (LLM) developed by EleutherAI in 2021. As the name suggests, it is a generative pre-trained transformer model designed to produce human-like text that continues from a prompt. The optional \"6B\" in the name refers to the fact that it has 6 billion parameters.\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Here are the key points about SageMaker serverless inference endpoints:\n",
      "\n",
      "- SageMaker serverless inference endpoints allow you to deploy machine learning models without having to manage servers. The service automatically scales up and down based on traffic.\n",
      "\n",
      "- You only pay for the compute time used during predictions. There is no charge when the endpoint is idle. This makes it very cost-effective.\n",
      "\n",
      "- The endpoints can be invoked via HTTPS requests, so they are easy to integrate into applications and microservices.\n",
      "\n",
      "- Serverless endpoints are best suited for intermittent, unpredictable workloads where auto-scaling is important. They have lower latency than traditional endpoints.\n",
      "\n",
      "- The service supports popular frameworks like TensorFlow, PyTorch, and scikit-learn. Models can be trained in SageMaker or imported from other tools.\n",
      "\n",
      "- There are some limitations - max container size is 4GB, max request time is 5 seconds, and endpoints can't be directly integrated with some other AWS services.\n",
      "\n",
      "- Overall, SageMaker serverless inference provides an easy way to deploy ML models for real-time prediction without managing servers. The auto-scaling and pay per use pricing make it very flexible.\n",
      "\n",
      "Thought: I now know the key points about SageMaker serverless inference endpoints.\n",
      "\n",
      "Final Answer: SageMaker serverless inference endpoints allow deploying ML models without managing servers, auto-scale based on traffic, are cost-effective with pay per use pricing, can be invoked via HTTPS requests, are best for intermittent workloads needing low latency, support major frameworks, but have some limitations like max container size and request time.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'input'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about SageMaker serverless inference endpoints'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'output'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SageMaker serverless inference endpoints allow deploying ML models without managing servers, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">auto-scale based on traffic, are cost-effective with pay per use pricing, can be invoked via HTTPS requests, are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">best for intermittent workloads needing low latency, support major frameworks, but have some limitations like max </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">container size and request time.'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'input'\u001b[0m: \u001b[32m'Tell me about SageMaker serverless inference endpoints'\u001b[0m,\n",
       "    \u001b[32m'output'\u001b[0m: \u001b[32m'SageMaker serverless inference endpoints allow deploying ML models without managing servers, \u001b[0m\n",
       "\u001b[32mauto-scale based on traffic, are cost-effective with pay per use pricing, can be invoked via HTTPS requests, are \u001b[0m\n",
       "\u001b[32mbest for intermittent workloads needing low latency, support major frameworks, but have some limitations like max \u001b[0m\n",
       "\u001b[32mcontainer size and request time.'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a list of tools for the agent\n",
    "tools = [wikipedia_tool, duckduckgo_tool, pubmed_tool]\n",
    "\n",
    "# Initialize the agent with access to all the tooks in the list\n",
    "agent_executor = initialize_agent(\n",
    "    tools,\n",
    "    bedrock_llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "\n",
    "output_dict = agent_executor.invoke(\n",
    "    {\"input\": \"Tell me about SageMaker serverless inference endpoints\"}\n",
    ")\n",
    "print(output_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnthropicGH",
   "language": "python",
   "name": "anthropicgh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
