{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d31d450-07f2-4338-a254-5dabb44cc86b",
   "metadata": {},
   "source": [
    "# Integrating LangChain Agents with Claude and Bedrock\n",
    "\n",
    "In this notebook, we integrate 3 external tools using LangChain Agent tools with Anthropic Claude as our LLM\n",
    "\n",
    "- Wikipedia (to answer general questions about people, history etc)\n",
    "- DuckduckGo search (to answer questions on current events)\n",
    "- A custom calculator tool to perform calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255309b5-46e0-4bf0-aa77-b9cd7dbc4f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install wikipedia duckduckgo-search --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c76d8d4c-f9ad-4524-b883-31e84403b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from rich import print\n",
    "from utils import get_inference_parameters, save_prompt\n",
    "\n",
    "%load_ext rich\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e8690-1448-487b-806b-f86f535b2206",
   "metadata": {},
   "source": [
    "### Initialize LLM\n",
    "\n",
    "We use `claude-instant` model for quicker responses. You may switch to `claude-v2` for larger context lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd74194-8132-4d07-9f7c-c157dbbb45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-west-2\"\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "# model_id = \"anthropic.claude-v2\"\n",
    "model_id = \"anthropic.claude-instant-v1\"\n",
    "model_kwargs = get_inference_parameters(\"anthropic\")\n",
    "\n",
    "# Instantial LLM\n",
    "llm = Bedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    "    region_name=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af25e071-ff18-4579-a8d4-35cd00ecb004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'max_tokens_to_sample'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'temperature'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'top_k'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">250</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'top_p'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.999</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'stop_sequences'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'\\n\\nHuman:'</span><span style=\"font-weight: bold\">]}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'max_tokens_to_sample'\u001b[0m: \u001b[1;36m1000\u001b[0m, \u001b[32m'temperature'\u001b[0m: \u001b[1;36m0.0\u001b[0m, \u001b[32m'top_k'\u001b[0m: \u001b[1;36m250\u001b[0m, \u001b[32m'top_p'\u001b[0m: \u001b[1;36m0.999\u001b[0m, \u001b[32m'stop_sequences'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'\\n\\nHuman:'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb27e8-45eb-4eb2-a50d-b0be7293bbfe",
   "metadata": {},
   "source": [
    "### Initialize tools\n",
    "\n",
    "- Wikipedia\n",
    "- Duckduckgo Search\n",
    "- Custom Math Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ecb958-1f67-4725-9eae-089e419071b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# Define the API wrapper for Wikipedia\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "\n",
    "# Define the Wikipedia tool with a description and the function to retrieve results from Wikipedia\n",
    "wikipedia_tool = Tool(\n",
    "    name=\"wikipedia\",\n",
    "    func=wikipedia.run,\n",
    "    description=\"Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\",\n",
    ")\n",
    "\n",
    "# Define the API wrapper for DuckDuckGo search\n",
    "duckduckgo_search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Define the DuckDuckGo tool using a description and the function to retrieve results from DuckDuckGo search\n",
    "duckduckgo_tool = Tool(\n",
    "    name=\"DuckDuckGoSearch\",\n",
    "    func=duckduckgo_search.run,\n",
    "    description=\"useful for when you need to answer questions about current events\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de6c06-28cb-48bb-a636-3c9f192bf934",
   "metadata": {},
   "source": [
    "#### Custom Math tool\n",
    "\n",
    "Here, we create a custom `math_chain` with custom `model_kwargs` and define this as a tool to be used by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e9df8b-1a43-473f-aede-aca3966d35eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableParallel</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnablePassthrough</span><span style=\"font-weight: bold\">()})</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Human: Given a question with a math problem, provide only a single line mathematical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expression that solves the problem in the following format. Don't solve the expression only create a parsable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expression.\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n\\nAssistant:\\n Here is</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an example response with a single line mathematical expression for solving a math </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">problem:\\n```text\\n37593**(1/5)\\n```\\n\\nHuman: {question}\\n\\nAssistant:\"</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Bedrock</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">client</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">botocore.client.BedrockRuntime</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x10da0e850</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">region_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'us-west-2'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'anthropic.claude-instant-v1'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model_kwargs</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'max_tokens_to_sample'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'temperature'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'top_k'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">250</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'top_p'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.999</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'stop_sequences'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'```output'</span><span style=\"font-weight: bold\">]</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mRunnableParallel\u001b[0m\u001b[1m(\u001b[0m\u001b[33msteps\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'question'\u001b[0m: \u001b[1;35mRunnablePassthrough\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mtemplate\u001b[0m=\u001b[32m\"Human\u001b[0m\u001b[32m: Given a question with a math problem, provide only a single line mathematical \u001b[0m\n",
       "\u001b[32mexpression that solves the problem in the following format. Don't solve the expression only create a parsable \u001b[0m\n",
       "\u001b[32mexpression.\\n```text\\n$\u001b[0m\u001b[32m{\u001b[0m\u001b[32m{\u001b[0m\u001b[32msingle line mathematical expression that solves the problem\u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n```\\n\\nAssistant:\\n Here is\u001b[0m\n",
       "\u001b[32man example response with a single line mathematical expression for solving a math \u001b[0m\n",
       "\u001b[32mproblem:\\n```text\\n37593**\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1/5\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nHuman: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mquestion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\nAssistant:\"\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mBedrock\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mclient\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mbotocore.client.BedrockRuntime\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x10da0e850\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[33mregion_name\u001b[0m=\u001b[32m'us-west-2'\u001b[0m,\n",
       "            \u001b[33mmodel_id\u001b[0m=\u001b[32m'anthropic.claude-instant-v1'\u001b[0m,\n",
       "            \u001b[33mmodel_kwargs\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'max_tokens_to_sample'\u001b[0m: \u001b[1;36m1000\u001b[0m,\n",
       "                \u001b[32m'temperature'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "                \u001b[32m'top_k'\u001b[0m: \u001b[1;36m250\u001b[0m,\n",
       "                \u001b[32m'top_p'\u001b[0m: \u001b[1;36m0.999\u001b[0m,\n",
       "                \u001b[32m'stop_sequences'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'```output'\u001b[0m\u001b[1m]\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Math Prompt Template\n",
    "math_template = \"\"\"Human: Given a question with a math problem, provide only a single line mathematical expression that solves the problem in the following format. Don't solve the expression only create a parsable expression.\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "\n",
    "Assistant:\n",
    " Here is an example response with a single line mathematical expression for solving a math problem:\n",
    "```text\n",
    "37593**(1/5)\n",
    "```\n",
    "\n",
    "Human: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "math_prompt = PromptTemplate.from_template(template=math_template)\n",
    "\n",
    "# Create a new LLM connection with custom `stop_sequence`\n",
    "math_model_kwargs = model_kwargs\n",
    "# Update the stop_seqeunce based on prompt_template\n",
    "math_model_kwargs[\"stop_sequences\"] = [\"```output\"]\n",
    "\n",
    "math_llm = Bedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=model_id,\n",
    "    model_kwargs=math_model_kwargs,\n",
    "    region_name=region,\n",
    ")\n",
    "\n",
    "# Create a custom Math chain that'll use LLM to determine the output\n",
    "math_chain = {\"question\": RunnablePassthrough()} | math_prompt | math_llm | StrOutputParser()\n",
    "\n",
    "# Define the tool\n",
    "math_tool = Tool.from_function(\n",
    "    func=math_chain.invoke,\n",
    "    name=\"Calculator\",\n",
    "    description=\"Useful for when you need to answer questions about math.\",\n",
    ")\n",
    "\n",
    "print(math_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0ec45-a149-4d56-a9b7-46156d32d5b1",
   "metadata": {},
   "source": [
    "### Prepare ReAct prompt for Agent\n",
    "\n",
    "For the Agent to utilize the tools defined in the prompt, we define a ReAct (**Re**ason and **Act**) prompt template that tells which tools are avialable and how to work through the question.\n",
    "\n",
    "First, we define a list of tools and then create text strings that define:\n",
    "1. `tools_string`: Formats text in `tool_name: tool_description \\n` format\n",
    "2. `tool_names` : Collect the list of tool names as defined in the tool definitions above.\n",
    "\n",
    "For simplicity, we break the prompt into 3 parts:\n",
    "- *Prefix:* prefix the prompt with `HUMAN_PROMPT` and fill with `tools_string`\n",
    "- *ReAct:* This portion of the prompt will replace `tool_names` to be used in the template\n",
    "- *Suffix:* Finally, we add the Question in xml tags and Assistant tags\n",
    "- **Final Prompt:** We concatenate all 3 portions above to form the final prompt\n",
    "\n",
    "*Optional:* Uncomment the code at the end of the cell save the prompt to disk as text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf4d6b1-035d-47e1-9460-94b6d6b1e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct list of tools for the agent\n",
    "tools = [wikipedia_tool, duckduckgo_tool, math_tool]\n",
    "\n",
    "# Prepare tools_string with `tool name: tool description` to inject into the prompt\n",
    "\n",
    "tools_string = \"\"\n",
    "tool_names = []  # collect tool names\n",
    "for tool in tools:\n",
    "    if isinstance(tool, Tool):\n",
    "        tool_names.append(tool.name)\n",
    "        tools_string += f\"{tool.name}: {tool.description}\\n\"\n",
    "\n",
    "# Optional: shuffle the tool_names list\n",
    "# import random\n",
    "# random.shuffle(tool_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b47bc9db-2653-49a5-be95-36fe97c93dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic_bedrock import HUMAN_PROMPT\n",
    "\n",
    "# Before initializing the agent we need to provide anthropic formatter prompt that tells the tools to use and how to use it\n",
    "prompt_prefix = \"\"\"You are helpful AI research assistant. Your task is to answer the user's question in <question> tags in a truthful manner.\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools_string}\n",
    "\"\"\"\n",
    "\n",
    "prompt_react = \"\"\"Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above\n",
    "Action: the action to take, should be one of {tool_names}\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question in <answer> tags\n",
    "\n",
    "\"\"\"\n",
    "# Replace tool_string\n",
    "prompt_prefix = f\"{HUMAN_PROMPT}{prompt_prefix.format(tools_string=tools_string)}\"\n",
    "# Replace tool_names\n",
    "prompt_react = prompt_react.format(tool_names=tool_names)\n",
    "\n",
    "# Tool names are hard\n",
    "prompt_suffix = \"\"\"Question: <question> {input} </question>\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "agent_prompt_template = f\"{prompt_prefix}{prompt_react}{prompt_suffix}\"\n",
    "\n",
    "# print(agent_prompt_template)\n",
    "# Optional: Save prompt to disk\n",
    "# prompt_save_path = Path('./prompts/react_agent_prompt_anthropic.txt')\n",
    "# print(f\"Saving prompt to: {str(prompt_save_path)}\")\n",
    "# save_prompt(agent_prompt_template, prompt_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ebbb5-3b5a-490a-bc40-26036764bad4",
   "metadata": {},
   "source": [
    "### Initialize Agent \n",
    "\n",
    "We pass in the list of tools, llm and `AgentType` to initialize the agent. We also assign the ReAct prompt to the llm used by the Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "652f5668-e179-4720-ba61-58c321877a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent with access to all the tools in the list\n",
    "agent_executor = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "\n",
    "# assign the prompt to the agent\n",
    "agent_executor.agent.llm_chain.prompt.template = agent_prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f14ef8-7abb-4ad5-a8a1-cd57a1ca5d72",
   "metadata": {},
   "source": [
    "### Execute (run) the agent with an input\n",
    "\n",
    "Extract the answer from `<answer>` tags from output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c127079-1921-43c2-8e99-6006b730d4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Here is my attempt at answering the question:\n",
      "\n",
      "Question: <question> How does Mixtral perform comparing to other SOTA models </question>\n",
      "\n",
      "Thought: I don't have direct knowledge of how Mixtral compares to other models. Let me search online for information.\n",
      "\n",
      "Action: Wikipedia \n",
      "Action Input: Mixtral\n",
      "\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, DuckDuckGoSearch, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m Let me try searching online instead.\n",
      "\n",
      "Action: DuckDuckGoSearch  \n",
      "Action Input: Mixtral model performance compared to other SOTA models\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mTo that end, Azure AI Studio provides a model benchmarking and evaluation subsystem, which is an invaluable tool for users to review and compare the performance of various AI models. The platform provides quality metrics for Azure OpenAI Service models and Llama 2 models such as Llama-2-7b, gpt-4, gpt-4-32k, and gpt-35-turbo. Mixtral 8x7b is an exciting new LLM released by Mistral, which sets a new standard for open-access models and outperforms GPT-3.5 across many benchmarks. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a ... Mixtral-8x7B Performance Metrics. The new model is designed to understand better and create text, a key feature for anyone looking to use AI for writing or communication tasks. It outperforms ... Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance ... In a move towards advancing artificial intelligence, Mistral AI, a pioneer in delivering cutting-edge open models, has unveiled Mixtral 8x7B. This high-quality sparse mixture-of-experts (SMoE) model with open weights marks a significant leap in the field. Steering away from conventional architectures and training paradigms, Mistral AI aims to ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Based on the search results, Mixtral is a new large language model released by Mistral AI that outperforms other SOTA models like GPT-3.5 and Llama 2 on many benchmarks while being faster and more efficient. It uses a novel sparse mixture-of-experts architecture.\n",
      "\n",
      "Final Answer: <answer>Based on the search results, Mixtral is a new large language model released by Mistral AI that outperforms other SOTA models like GPT-3.5 and Llama 2 on many benchmarks while being faster and more efficient. It uses a novel sparse mixture-of-experts architecture.</answer>\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Based on the search results, Mixtral is a new large language model released by Mistral AI that outperforms other \n",
       "SOTA models like GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span> and Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> on many benchmarks while being faster and more efficient. It uses a novel \n",
       "sparse mixture-of-experts architecture.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Based on the search results, Mixtral is a new large language model released by Mistral AI that outperforms other \n",
       "SOTA models like GPT-\u001b[1;36m3.5\u001b[0m and Llama \u001b[1;36m2\u001b[0m on many benchmarks while being faster and more efficient. It uses a novel \n",
       "sparse mixture-of-experts architecture.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# question = \"What is Mixture of Experts in Machine Learning\"  # Test wikipedia\n",
    "# question = \"What is FlashAttention? What are it's benefits\"\n",
    "# question = \"What is Mixtral and how is it different from other Mistral models\"\n",
    "question = \"How does Mixtral perform comparing to other SOTA models\"  # Test web search\n",
    "# question = \"What is conciousness\"\n",
    "# question = \"Get me the biography of Chester Bennington. Include his birth date, city and country\"\n",
    "# question = \"Amazon SageMaker launch year multiplied by 2\"  # Test math_tool\n",
    "\n",
    "output = agent_executor.run(question)\n",
    "\n",
    "# extract answer from <answer> tags\n",
    "final_output = re.findall(r\"<answer>(.*?)</answer>\", output, flags=re.DOTALL)[0]\n",
    "print(final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnthropicGH",
   "language": "python",
   "name": "anthropicgh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
