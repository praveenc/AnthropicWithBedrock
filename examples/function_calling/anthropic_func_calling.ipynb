{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8347bc-fe5f-481f-bfe2-580bab7752ef",
   "metadata": {},
   "source": [
    "# Function calling with Anthropic Claude LLM and Amazon Bedrock\n",
    "\n",
    "In this notebook we explore how to do function calling with Anthropic Claude LLMs on Amazon Bedrock.\n",
    "To keep it simple, we use raw propmts to acheive function calling.\n",
    "\n",
    "## Why function calling\n",
    "\n",
    "For example, a user might want to ask Claude to get the weather forecast. Claude is unable to perform this action by itself, but when using with the function calling prompt template, Claude can decide to call a function named get_weather(location: str) that has been described in the prompt template.\n",
    "\n",
    "Through the function calling prompt, customers can now describe functions to Claude and have the model intelligently choose to use the functions to answer user questions.\n",
    "\n",
    "\n",
    "Anthropic has a Python API library [`anthropic-bedrock`](https://github.com/anthropics/anthropic-bedrock-python) specifically tailored for Bedrock. Refer to [anthropic-bedrock](https://github.com/anthropics/anthropic-bedrock-python) github repo for more info.\n",
    "\n",
    "This notebook is tested with Bedrock modelId **`anthropic.claude-v2`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176c158b-bdaf-42a7-b45b-97e2a1f33ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all prerequisites to run this notebook\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from types import FunctionType\n",
    "\n",
    "import boto3\n",
    "import tools\n",
    "from anthropic_bedrock import AI_PROMPT, HUMAN_PROMPT, AnthropicBedrock\n",
    "from IPython.display import Markdown\n",
    "from loguru import logger\n",
    "from rich import print\n",
    "from rich.status import Status\n",
    "\n",
    "!pip install -r requirements.txt --quiet\n",
    "\n",
    "# load rich extension for pretty printing with format.\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd7a082-9cbd-4bc1-9a0f-e8af897d3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febcc1d-f880-42cc-bd5f-edac586c35df",
   "metadata": {},
   "source": [
    "### Create Anthropic client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e716cea-e1bf-47a2-8da7-857465022c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "creds = session.get_credentials()\n",
    "\n",
    "client = AnthropicBedrock(\n",
    "    aws_access_key=creds.access_key,\n",
    "    aws_secret_key=creds.secret_key,\n",
    "    aws_session_token=creds.token,\n",
    "    aws_region=session.region_name,\n",
    ")\n",
    "\n",
    "model_id = \"anthropic.claude-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a83beb-7a81-423f-b250-f9c45bfa3ff4",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Here we define the following helper functions.\n",
    "\n",
    "1. `create_prompt` function returns the function calling prompt with function definitions along with few-shot examples\n",
    "2. `add_tools` function gets all function definitions in a XML formatted string\n",
    "3. `execute_function` extracts function name and function args from LLM output, executes the function and returns the function output in `<function_result></function_result>` tags\n",
    "4. `invoke_llm` invokes **`anthropic.claude-v2`** and returns `completion`, `stop_reason` and `stop_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c909064-21a3-4aef-a872-1abb310b7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(tools_string, user_input):\n",
    "    template = f\"\"\"\n",
    "Human: You are a research assistant AI that has been equipped with the following function(s) to help you answer a <question>. Your goal is to answer the user's question to the best of your ability, using the function(s) to gather more information if necessary to better answer the question. The result of a function call will be added to the conversation history as an observation.\n",
    "\n",
    "Here are the only function(s) I have provided you with:\n",
    "\n",
    "<functions> {tools_string} </functions>\n",
    "\n",
    "Note that the function arguments have been listed in the order that they should be passed into the function.\n",
    "\n",
    "Do not modify or extend the provided functions under any circumstances. For example, calling get_current_temp() with additional parameters would be considered modifying the function which is not allowed. Please use the functions only as defined.\n",
    "\n",
    "DO NOT use any functions that I have not equipped you with.\n",
    "\n",
    "To call a function, output <function_call>insert specific function</function_call>. You will receive a <function_result> in response to your call that contains information that you can use to better answer the question.\n",
    "\n",
    "Here is an example of how you would correctly answer a question using a <function_call> and the corresponding <function_result>. Notice that you are free to think before deciding to make a <function_call> in the <scratchpad>:\n",
    "\n",
    "<example>\n",
    "<functions>\n",
    "<function>\n",
    "<function_name>get_current_temp</function_name>\n",
    "<function_description>Gets the current temperature for a given city.</function_description>\n",
    "<required_argument>city (str): The name of the city to get the temperature for.</required_argument>\n",
    "<returns>int: The current temperature in degrees Fahrenheit.</returns>\n",
    "<raises>ValueError: If city is not a valid city name.</raises>\n",
    "<example_call>get_current_temp(city=\"New York\")</example_call>\n",
    "</function>\n",
    "</functions>\n",
    "\n",
    "<question>What is the current temperature in San Francisco?</question>\n",
    "\n",
    "<scratchpad>I do not have access to the current temperature in San Francisco so I should use a function to gather more information to answer this question. I have been equipped with the function get_current_temp that gets the current temperature for a given city so I should use that to gather more information.\n",
    "\n",
    "I have double checked and made sure that I have been provided the get_current_temp function. I have double checked that the <function_call> tags contain only the function call and nothing else.\n",
    "</scratchpad>\n",
    "\n",
    "<function_call>get_current_temp(city=\"San Francisco\")</function_call>\n",
    "\n",
    "<function_result>71</function_result>\n",
    "\n",
    "<answer>The current temperature in San Francisco is 71 degrees Fahrenheit.</answer>\n",
    "</example>\n",
    "\n",
    "This example shows how you should respond to questions that cannot be answered using information from the functions you are provided with. Remember, DO NOT use any functions that I have not provided you with.\n",
    "\n",
    "Remember, your goal is to answer the user's question to the best of your ability, using only the function(s) provided to gather more information if necessary to better answer the question.\n",
    "\n",
    "Do not modify or extend the provided functions under any circumstances. For example, calling get_current_temp() with additional parameters would be modifying the function which is not allowed. Please use the functions only as defined.\n",
    "\n",
    "The result of a function call will be added to the conversation history as an observation. If necessary, you can make multiple function calls and use all the functions I have equipped you with. Let's create a plan and then execute the plan. Double check your plan to make sure you don't call any functions that I haven't provided. Always return your final answer within  <answer></answer> tags.\n",
    "\n",
    "DO NOT output any preamble like 'based on calling the provided functions ...'. If <function_result> contains items in a list or tuple then format them using markdown list. Do NOT include <function_result> tags in the output.\n",
    "Just answer the <question> in a direct manner.\n",
    "\n",
    "The question to answer is <question> {user_input} </question>\n",
    "\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "    return template\n",
    "\n",
    "\n",
    "def add_tools():\n",
    "    tools_string = \"\"\n",
    "    for tool_spec in tools.list_of_function_specs:\n",
    "        tools_string += tool_spec\n",
    "    return tools_string\n",
    "\n",
    "\n",
    "def execute_function(func_text: str):\n",
    "    def format_result(output):\n",
    "        return f\"\"\"<function_result>{output}</function_result>\"\"\"\n",
    "\n",
    "    func_name = re.search(r\"([^\\(]+)\\(\", func_text).group(1)\n",
    "    func = getattr(tools, func_name)  # Get reference to function from mytools\n",
    "    func_text = func_text.replace(\"()\", \"\")\n",
    "    kwargs = {}\n",
    "    if \"(\" in func_text and \")\" in func_text:\n",
    "        args_str = re.search(r\"\\(([^\\)]+)\\)\", func_text).group(1)\n",
    "        for arg in args_str.split(\",\"):\n",
    "            parts = arg.split(\"=\")  # Split by =\n",
    "            key = parts[0].strip()  # Strip whitespace in param name\n",
    "            value = parts[1]\n",
    "            kwargs[key] = value\n",
    "\n",
    "    # logger.info(f\"Extracted function_name: {func_name}\")\n",
    "    # logger.info(f\"Extracted args: {kwargs}\")\n",
    "    assert isinstance(func, FunctionType)  # Check that it is callable\n",
    "    if len(kwargs) >= 1:\n",
    "        result = format_result(\n",
    "            func(**kwargs)\n",
    "        )  # Call the function and format the result\n",
    "    else:\n",
    "        result = format_result(func())\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_function_result(partial_completion):\n",
    "    \"\"\"extracts function name and arguments, executes the func\"\"\"\n",
    "    # LLM outputs <function_call>get_lat_long(place=\"NYC\")\n",
    "    # We extract text after <function_call> and call execute_function\n",
    "    start_index = partial_completion.find(\"<function_call>\")\n",
    "    function_result = \"\"\n",
    "    if start_index != -1:\n",
    "        extracted_text = partial_completion[start_index + 15 :].strip()\n",
    "        # logger.info(f\"Extracted Text: {extracted_text}\")\n",
    "        function_result = execute_function(extracted_text)\n",
    "    return function_result\n",
    "\n",
    "\n",
    "def invoke_llm(\n",
    "    client,\n",
    "    prompt: str,\n",
    "    modelId: str = model_id,\n",
    "    max_tokens: int = 1000,\n",
    "    temperature: float = 0.0,\n",
    "):\n",
    "    try:\n",
    "        partial_completion = client.completions.create(\n",
    "            prompt=prompt,\n",
    "            stop_sequences=[\"\\n\\nHuman:\", \"</function_call>\"],\n",
    "            model=modelId,\n",
    "            max_tokens_to_sample=max_tokens,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return (\n",
    "            partial_completion.completion,\n",
    "            partial_completion.stop_reason,\n",
    "            partial_completion.stop,\n",
    "        )\n",
    "    except anthropic_bedrock.APIConnectionError as e:\n",
    "        logger.error(\"The server could not be reached\")\n",
    "        logger.error(\n",
    "            e.__cause__\n",
    "        )  # an underlying Exception, likely raised within httpx.\n",
    "    except anthropic_bedrock.RateLimitError as e:\n",
    "        logger.error(\"A 429 status code was received; we should back off a bit.\")\n",
    "    except anthropic_bedrock.APIStatusError as e:\n",
    "        logger.error(\"Another non-200-range status code was received\")\n",
    "        logger.error(e.status_code)\n",
    "        logger.error(e.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e14db-b0fc-490d-9c34-92e92f78ce4b",
   "metadata": {},
   "source": [
    "### Create prompt\n",
    "\n",
    "1. Enter question to be answered\n",
    "2. get the list of function definitions (`add_tools` function)\n",
    "3. call `create_prompt` with functions\n",
    "\n",
    ">**NOTE:** [tools.py](./tools.py) currently defines 3 functions but has function definitions only for 2. Because `get_weather` calls `get_weather_code` function. `get_weather_code` is not provided as a function definition. Refer to [tools.py](./tools.py) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac314afd-8e12-4035-81b2-49815fc7a1a9",
   "metadata": {},
   "source": [
    "### Invoke LLM in a loop until desired `stop_sequence`\n",
    "\n",
    "To invoke `Claudev2`, we call the `invoke_llm` function. This function returns 3 values:\n",
    "- `partial_completion` (output from the LLM)\n",
    "- `stop_reason`\n",
    "- `stop_sequence`\n",
    "   - a new stop sequence called `</function_call>` is added when calling `client.completions.create` in `invoke_llm` function. We use this `stop_sequence` to determine when to stop invoking the LLM.\n",
    " \n",
    "#### Invoke LLM in a loop until `HUMAN_PROMPT`\n",
    "\n",
    "Here's what's happening below:\n",
    "1. From the output find string `<function_call>` and strip this text (`extracted_text`)\n",
    "2. Call `execute_function` with `extracted_text`\n",
    "   - `execute_function` returns output formatted in `<function_result>` tags\n",
    "3. We now, need to reformat the original prompt with the `function_result` and add `AI_PROMPT` i.e `\\n\\nAssistant:` to the end of the prompt.\n",
    "4. call `invoke_llm` again and repeat this until `stop_sequence` matches `HUMAN_PROMPT` i.e., `\\n\\nHuman:`, this marks the end of the loop.\n",
    "5. Finally, print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef09a4c9-595a-4f5c-917f-4101ebeae5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-25 12:20:08.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools\u001b[0m:\u001b[36mget_weather\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m{'temperature': 69.38150024414062, 'conditions': 'Clear sky'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-25 12:20:11.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mTotal Tokens: 4140\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Question:</span> <span style=\"font-weight: bold\">What is the weather in Gainesville</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mQuestion:\u001b[0m \u001b[1mWhat is the weather in Gainesville\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">answer</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The weather in Gainesville is:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- Temperature: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">69.38</span><span style=\"color: #000000; text-decoration-color: #000000\"> F</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- Conditions: Clear sky</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">answer</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       " \u001b[1m<\u001b[0m\u001b[1;95manswer\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39mThe weather in Gainesville is:\u001b[0m\n",
       "\n",
       "\u001b[39m- Temperature: \u001b[0m\u001b[1;36m69.38\u001b[0m\u001b[39m F\u001b[0m\n",
       "\u001b[39m- Conditions: Clear sky\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95manswer\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "question = \"What is the weather in Gainesville\"  # Question we need answer for\n",
    "function_defs = add_tools()  # Get list of function definitions\n",
    "prompt = create_prompt(function_defs, question)  # Get prompt\n",
    "\n",
    "with console.status(\"[magenta]Invoking LLM...\") as status:\n",
    "    token_count = client.count_tokens(prompt)  # Keep track of total tokens\n",
    "    status.update(status=\"[bold blue] Invoking LLM ...\", spinner=\"earth\")\n",
    "    partial_completion, stop_reason, stop_seq = invoke_llm(client, prompt)\n",
    "    token_count = client.count_tokens(prompt + partial_completion)\n",
    "    status.update(\n",
    "        status=f\"[bold blue] Invoking LLM Token count: {token_count} ...\",\n",
    "        spinner=\"earth\",\n",
    "    )\n",
    "    while stop_seq == \"</function_call>\":\n",
    "        start_index = partial_completion.find(\"<function_call>\")\n",
    "        if start_index != -1:\n",
    "            extracted_text = partial_completion[start_index + 15 :].strip()\n",
    "            status.update(\n",
    "                status=f\"executing function: [green]{extracted_text}[/green]\",\n",
    "                spinner=\"earth\",\n",
    "            )\n",
    "            function_result = execute_function(extracted_text)\n",
    "            # logger.info(f\"function: {extracted_text} result: {function_result}\")\n",
    "            prompt += f\"{partial_completion}{stop_seq}\\n{function_result}{AI_PROMPT}\"\n",
    "            partial_completion, stop_reason, stop_seq = invoke_llm(client, prompt)\n",
    "            token_count += client.count_tokens(prompt + partial_completion)\n",
    "\n",
    "if stop_seq == HUMAN_PROMPT:\n",
    "    status.stop()\n",
    "    logger.info(f\"Total Tokens: {token_count}\")\n",
    "    print(f\"[green]Question:[/green] [b]{question}[/b]\\n\")\n",
    "    print(f\"{partial_completion}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "nlp311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
