{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8347bc-fe5f-481f-bfe2-580bab7752ef",
   "metadata": {},
   "source": [
    "# Function calling with Anthropic Claude LLM and Amazon Bedrock\n",
    "\n",
    "In this notebook we explore how to do function calling with Anthropic Claude LLMs on Amazon Bedrock.\n",
    "To keep it simple, we use raw propmts to acheive function calling.\n",
    "\n",
    "## Why function calling\n",
    "\n",
    "For example, a user might want to ask Claude to get the weather forecast. Claude is unable to perform this action by itself, but when using with the function calling prompt template, Claude can decide to call a function named get_weather(location: str) that has been described in the prompt template.\n",
    "\n",
    "Through the function calling prompt, customers can now describe functions to Claude and have the model intelligently choose to use the functions to answer user questions.\n",
    "\n",
    "\n",
    "Anthropic has a Python API library [`anthropic-bedrock`](https://github.com/anthropics/anthropic-bedrock-python) specifically tailored for Bedrock. Refer to [anthropic-bedrock](https://github.com/anthropics/anthropic-bedrock-python) github repo for more info.\n",
    "\n",
    "This notebook is tested with Bedrock modelId **`anthropic.claude-v2`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176c158b-bdaf-42a7-b45b-97e2a1f33ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all prerequisites to run this notebook\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cff96a9-016f-4389-8788-086db04e7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rich extension for pretty printing with format.\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd7a082-9cbd-4bc1-9a0f-e8af897d3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from types import FunctionType\n",
    "from rich import print\n",
    "import boto3\n",
    "import time\n",
    "from loguru import logger\n",
    "from anthropic_bedrock import AnthropicBedrock, HUMAN_PROMPT, AI_PROMPT\n",
    "from IPython.display import Markdown\n",
    "from rich.status import Status\n",
    "import tools\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febcc1d-f880-42cc-bd5f-edac586c35df",
   "metadata": {},
   "source": [
    "### Create Anthropic client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e716cea-e1bf-47a2-8da7-857465022c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "creds = session.get_credentials()\n",
    "\n",
    "client = AnthropicBedrock(\n",
    "    aws_access_key=creds.access_key,\n",
    "    aws_secret_key=creds.secret_key,\n",
    "    aws_session_token=creds.token,\n",
    "    aws_region=session.region_name,\n",
    ")\n",
    "\n",
    "model_id = \"anthropic.claude-v2\"\n",
    "\n",
    "# Optional code: anthropic-bedrock module has `count_tokens` to count the number of tokens passed to the LLM\n",
    "# prompt = f\"{HUMAN_PROMPT} how does a court case get to the Supreme Court? {AI_PROMPT}\"\n",
    "# tokens = client.count_tokens(prompt)\n",
    "# logger.info(f\" Prompt: {prompt} has {tokens} # tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a83beb-7a81-423f-b250-f9c45bfa3ff4",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Here we define the following helper functions.\n",
    "\n",
    "1. `create_prompt` function returns the function calling prompt with function definitions along with few-shot examples\n",
    "2. `add_tools` function gets all function definitions in a XML formatted string\n",
    "3. `execute_function` extracts function name and function args from LLM output, executes the function and returns the function output in `<function_result></function_result>` tags\n",
    "4. `invoke_llm` invokes **`anthropic.claude-v2`** and returns `completion`, `stop_reason` and `stop_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c909064-21a3-4aef-a872-1abb310b7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(tools_string, user_input):\n",
    "    template = f\"\"\"\n",
    "Human: You are a research assistant AI that has been equipped with the following function(s) to help you answer a <question>. Your goal is to answer the user's question to the best of your ability, using the function(s) to gather more information if necessary to better answer the question. The result of a function call will be added to the conversation history as an observation.\n",
    "\n",
    "Here are the only function(s) I have provided you with:\n",
    "\n",
    "<functions>\n",
    "{tools_string}\n",
    "</functions>\n",
    "\n",
    "Note that the function arguments have been listed in the order that they should be passed into the function.\n",
    "\n",
    "Do not modify or extend the provided functions under any circumstances. For example, calling get_current_temp() with additional parameters would be considered modifying the function which is not allowed. Please use the functions only as defined.\n",
    "\n",
    "DO NOT use any functions that I have not equipped you with.\n",
    "\n",
    "To call a function, output <function_call>insert specific function</function_call>. You will receive a <function_result> in response to your call that contains information that you can use to better answer the question.\n",
    "\n",
    "Here is an example of how you would correctly answer a question using a <function_call> and the corresponding <function_result>. Notice that you are free to think before deciding to make a <function_call> in the <scratchpad>:\n",
    "\n",
    "<example>\n",
    "<functions>\n",
    "<function>\n",
    "<function_name>get_current_temp</function_name>\n",
    "<function_description>Gets the current temperature for a given city.</function_description>\n",
    "<required_argument>city (str): The name of the city to get the temperature for.</required_argument>\n",
    "<returns>int: The current temperature in degrees Fahrenheit.</returns>\n",
    "<raises>ValueError: If city is not a valid city name.</raises>\n",
    "<example_call>get_current_temp(city=\"New York\")</example_call>\n",
    "</function>\n",
    "</functions>\n",
    "\n",
    "<question>What is the current temperature in San Francisco?</question>\n",
    "\n",
    "<scratchpad>I do not have access to the current temperature in San Francisco so I should use a function to gather more information to answer this question. I have been equipped with the function get_current_temp that gets the current temperature for a given city so I should use that to gather more information.\n",
    "\n",
    "I have double checked and made sure that I have been provided the get_current_temp function. I have double checked that the <function_call> tags contain only the function call and nothing else.\n",
    "</scratchpad>\n",
    "\n",
    "<function_call>get_current_temp(city=\"San Francisco\")</function_call>\n",
    "\n",
    "<function_result>71</function_result>\n",
    "\n",
    "<answer>The current temperature in San Francisco is 71 degrees Fahrenheit.</answer>\n",
    "</example>\n",
    "\n",
    "This example shows how you should respond to questions that cannot be answered using information from the functions you are provided with. Remember, DO NOT use any functions that I have not provided you with.\n",
    "\n",
    "Remember, your goal is to answer the user's question to the best of your ability, using only the function(s) provided to gather more information if necessary to better answer the question.\n",
    "\n",
    "Do not modify or extend the provided functions under any circumstances. For example, calling get_current_temp() with additional parameters would be modifying the function which is not allowed. Please use the functions only as defined.\n",
    "\n",
    "The result of a function call will be added to the conversation history as an observation. If necessary, you can make multiple function calls and use all the functions I have equipped you with. Let's create a plan and then execute the plan. Double check your plan to make sure you don't call any functions that I haven't provided. Always return your final answer within  <answer></answer> tags.\n",
    "\n",
    "DO NOT output any preamble like 'based on calling the provided functions ...'. If <function_result> contains items in a list or tuple then format them using markdown list. Do NOT include <function_result> tags in the output.\n",
    "Just answer the <question> in a direct manner.\n",
    "\n",
    "The question to answer is <question>{user_input}</question>\n",
    "\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "    return template\n",
    "\n",
    "\n",
    "def add_tools():\n",
    "    tools_string = \"\"\n",
    "    for tool_spec in tools.list_of_function_specs:\n",
    "        tools_string += tool_spec\n",
    "    return tools_string\n",
    "\n",
    "\n",
    "def execute_function(func_text: str):\n",
    "    def format_result(output):\n",
    "        return f\"\"\"<function_result>{output}</function_result>\"\"\"\n",
    "\n",
    "    func_name = re.search(r\"([^\\(]+)\\(\", func_text).group(1)\n",
    "    func = getattr(tools, func_name)  # Get reference to function from mytools\n",
    "    func_text = func_text.replace(\"()\", \"\")\n",
    "    kwargs = {}\n",
    "    if \"(\" in func_text and \")\" in func_text:\n",
    "        args_str = re.search(r\"\\(([^\\)]+)\\)\", func_text).group(1)\n",
    "        for arg in args_str.split(\",\"):\n",
    "            parts = arg.split(\"=\")  # Split by =\n",
    "            key = parts[0].strip()  # Strip whitespace in param name\n",
    "            value = parts[1]\n",
    "            kwargs[key] = value\n",
    "\n",
    "    # logger.info(f\"Extracted function_name: {func_name}\")\n",
    "    # logger.info(f\"Extracted args: {kwargs}\")\n",
    "    assert isinstance(func, FunctionType)  # Check that it is callable\n",
    "    if len(kwargs) >= 1:\n",
    "        result = format_result(\n",
    "            func(**kwargs)\n",
    "        )  # Call the function and format the result\n",
    "    else:\n",
    "        result = format_result(func())\n",
    "    return result\n",
    "\n",
    "\n",
    "def invoke_llm(\n",
    "    client,\n",
    "    prompt: str,\n",
    "    modelId: str = model_id,\n",
    "    max_tokens: int = 1000,\n",
    "    temperature: float = 0.0,\n",
    "):\n",
    "    try:\n",
    "        partial_completion = client.completions.create(\n",
    "            prompt=prompt,\n",
    "            stop_sequences=[\"\\n\\nHuman:\", \"</function_call>\"],\n",
    "            model=modelId,\n",
    "            max_tokens_to_sample=max_tokens,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return (\n",
    "            partial_completion.completion,\n",
    "            partial_completion.stop_reason,\n",
    "            partial_completion.stop,\n",
    "        )\n",
    "    except anthropic_bedrock.APIConnectionError as e:\n",
    "        logger.error(\"The server could not be reached\")\n",
    "        logger.error(\n",
    "            e.__cause__\n",
    "        )  # an underlying Exception, likely raised within httpx.\n",
    "    except anthropic_bedrock.RateLimitError as e:\n",
    "        logger.error(\"A 429 status code was received; we should back off a bit.\")\n",
    "    except anthropic_bedrock.APIStatusError as e:\n",
    "        logger.error(\"Another non-200-range status code was received\")\n",
    "        logger.error(e.status_code)\n",
    "        logger.error(e.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e14db-b0fc-490d-9c34-92e92f78ce4b",
   "metadata": {},
   "source": [
    "### Create prompt\n",
    "\n",
    "1. Enter question to be answered\n",
    "2. get the list of function definitions (`add_tools` function)\n",
    "3. call `create_prompt` with functions\n",
    "\n",
    ">**NOTE:** [tools.py](./tools.py) currently defines 3 functions but has function definitions only for 2. Because `get_weather` calls `get_weather_code` function. `get_weather_code` is not provided as a function definition. Refer to [tools.py](./tools.py) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc8be792-ca1b-4bd1-a75f-d4cd73298528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-22 13:13:51.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mFunctions: <function>\n",
      "<function_name>get_weather</function_name>\n",
      "<function_description>Returns weather data for a given latitude and longitude.</function_description>\n",
      "<required_argument>latitude (str): Latitude coordinates for the Name of the city or Airport code</required_argument>\n",
      "<required_argument>longitude (str): Longitude coordinates for the Name of the city or Airport code</required_argument>\n",
      "<optional_argument>units (str): Temperature units, either \"F\" for Fahrenheit or \"C\" for Celsius, default \"F\"</optional_argument>\n",
      "<returns> temperature (float): Current temperature in Fahrenheit - Conditions (str): Short text description of weather conditions</returns>\n",
      "<example_call>get_weather(latitude=\"52.52\", longitude=\"-122.419998\", units=\"F\")</example_call>\n",
      "</function>\n",
      "<function>\n",
      "<function_name>get_lat_long</function_name>\n",
      "<function_description>Returns the latitude and longitude for a given place name.</function_description>\n",
      "<required_argument>place (str): Name of the city or Airport code to geocode and get coordinates for.</required_argument>\n",
      "<returns>latitude (str): Latitude coordinates of the place - longitude (str): Longitude coordinates of the place</returns>\n",
      "<raises>ValueError: If invalid location provided</raises>\n",
      "<example_call>get_lat_long(place=\"Seattle\")</example_call>\n",
      "</function>\n",
      "\u001b[0m\n",
      "\u001b[32m2023-11-22 13:13:51.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mNo. Tokens: 1230\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the weather in Los Angeles\"  # Question we need answer for\n",
    "function_defs = add_tools()\n",
    "logger.info(f\"Functions: {function_defs}\")\n",
    "prompt = create_prompt(function_defs, question)\n",
    "token_count = client.count_tokens(prompt)\n",
    "logger.info(f\"No. Tokens: {token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac314afd-8e12-4035-81b2-49815fc7a1a9",
   "metadata": {},
   "source": [
    "### Invoke LLM in a loop until desired `stop_sequence`\n",
    "\n",
    "To invoke `Claudev2`, we call the `invoke_llm` function. This function returns 3 values:\n",
    "- `partial_completion` (output from the LLM)\n",
    "- `stop_reason`\n",
    "- `stop_sequence`\n",
    "   - a new stop sequence called `</function_call>` is added when calling `client.completions.create` in `invoke_llm` function. We use this `stop_sequence` to determine when to stop invoking the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "166e6e7c-f5fa-43ef-81e7-5fe2c63cfd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-22 13:14:02.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPartial completion:  <scratchpad>\n",
      "To answer the question \"What is the weather in Los Angeles\", I need to:\n",
      "1. Get the latitude and longitude coordinates for Los Angeles using the get_lat_long() function.\n",
      "2. Use the latitude and longitude to get the current weather data for Los Angeles using the get_weather() function.\n",
      "</scratchpad>\n",
      "\n",
      "<function_call>get_lat_long(place=\"Los Angeles\")\u001b[0m\n",
      "\u001b[32m2023-11-22 13:14:02.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mStop Reason: stop_sequence, Stop Seq: </function_call>\u001b[0m\n",
      "\u001b[32m2023-11-22 13:14:02.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mNo. Tokens: 1322\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "partial_completion, stop_reason, stop_seq = invoke_llm(client, prompt)\n",
    "token_count = client.count_tokens(prompt + partial_completion)\n",
    "logger.info(f\"Partial completion: {partial_completion}\")\n",
    "logger.info(f\"Stop Reason: {stop_reason}, Stop Seq: {stop_seq}\")\n",
    "logger.info(f\"No. Tokens: {token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b27c1-7845-41bf-92ab-88bd185a285d",
   "metadata": {},
   "source": [
    "#### Invoke LLM in a loop until `HUMAN_PROMPT`\n",
    "\n",
    "Here's what's happening below:\n",
    "1. From the output find string `<function_call>` and strip this text (`extracted_text`)\n",
    "2. Call `execute_function` with `extracted_text`\n",
    "   - `execute_function` returns output formatted in `<function_result>` tags\n",
    "3. We now, need to reformat the original prompt with the `function_result` and add `AI_PROMPT` i.e `\\n\\nAssistant:` to the end of the prompt.\n",
    "4. call `invoke_llm` again and repeat this until `stop_sequence` matches `HUMAN_PROMPT` i.e., `\\n\\nHuman:`, this marks the end of the loop.\n",
    "5. Finally, print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef09a4c9-595a-4f5c-917f-4101ebeae5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-22 13:14:07.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools\u001b[0m:\u001b[36mget_weather\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m{'temperature': 81.46039581298828, 'conditions': 'Clear sky'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-22 13:14:13.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mTotal Tokens: 1456\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Question:</span> <span style=\"font-weight: bold\">What is the weather in Los Angeles</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mQuestion:\u001b[0m \u001b[1mWhat is the weather in Los Angeles\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">answer</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The weather in Los Angeles is:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- Temperature: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81.5</span><span style=\"color: #000000; text-decoration-color: #000000\"> F</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- Conditions: Clear sky</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">answer</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       " \u001b[1m<\u001b[0m\u001b[1;95manswer\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39mThe weather in Los Angeles is:\u001b[0m\n",
       "\n",
       "\u001b[39m- Temperature: \u001b[0m\u001b[1;36m81.5\u001b[0m\u001b[39m F\u001b[0m\n",
       "\u001b[39m- Conditions: Clear sky\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95manswer\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with Status(\"Invoking LLM...\", spinner=\"dots\") as status:\n",
    "    while True:\n",
    "        if stop_seq == \"</function_call>\":\n",
    "            start_index = partial_completion.find(\"<function_call>\")\n",
    "            if start_index != -1:\n",
    "                extracted_text = partial_completion[start_index + 15 :].strip()\n",
    "                # logger.info(f\"Extracted Text: {extracted_text}\")\n",
    "                function_result = execute_function(extracted_text)\n",
    "                prompt += (\n",
    "                    f\"{partial_completion}{stop_seq}\\n{function_result}{AI_PROMPT}\"\n",
    "                )\n",
    "                partial_completion, stop_reason, stop_seq = invoke_llm(client, prompt)\n",
    "                token_count = client.count_tokens(prompt + partial_completion)\n",
    "                # logger.info(f\"No. Tokens: {client.count_tokens(prompt)}\")\n",
    "                # logger.info(f\"stop_seq: {stop_seq}\")\n",
    "                status.update()\n",
    "                # time.sleep(0.25)\n",
    "                # logger.info(\"sleeping for 0.25 secs\")\n",
    "        if stop_seq == HUMAN_PROMPT:\n",
    "            # logger.info(f\"stop_seq: {stop_seq}\")\n",
    "            status.stop()\n",
    "            break\n",
    "\n",
    "    if stop_seq == HUMAN_PROMPT:\n",
    "        if stop_reason == \"stop_sequence\" and stop_seq == HUMAN_PROMPT:\n",
    "            logger.info(f\"Total Tokens: {token_count}\")\n",
    "            print(f\"[green]Question:[/green] [b]{question}[/b]\\n\")\n",
    "            print(f\"{partial_completion}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "nlp311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
