{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1574d1b-6cb4-4336-b2ba-d2f453be9bb0",
   "metadata": {},
   "source": [
    "# Question Answering on LangChain blogs (using MultiQuery Retriever)\n",
    "\n",
    "In this notebook we explore `MultiQueryRetriever` (MQR) to enhance output quality of RAG documents.\n",
    "\n",
    "Idea here is to generate upto 5 questions by rephrasing the original question without changing the context of the question.\n",
    "\n",
    "We then build a `MultiQueryRetriever` with our current retriever and llm to rephrase questions accordingly.\n",
    "\n",
    "Finally, whilst invoking the RAG chain, we use the MQR as our retriever.\n",
    "\n",
    "We'll QnA on a few blog post from [LangChain blogs](https://blog.langchain.dev/rss/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c72cdb73-0a40-4450-a989-c4946d370211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U feedparser rich --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa5a24a0-6147-46d2-8d70-63e9bd74ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + \"/utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3a310d-71be-4838-9f07-9cdead08d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "import boto3\n",
    "from anthropic_bedrock import AI_PROMPT, HUMAN_PROMPT\n",
    "from rich import print\n",
    "from utils import get_inference_parameters, get_model_ids\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext rich\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bcd492-3eaa-4d91-ae33-63928bdf9780",
   "metadata": {},
   "source": [
    "### Instantiate LLM and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e45a295-f4a2-4cc9-8357-dad2102a7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "region = \"us-west-2\"\n",
    "b_client = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "model_kwargs = get_inference_parameters(\n",
    "    \"anthropic\"\n",
    ")  # We need pass in model_kwargs for a model\n",
    "# llm_model_id = \"anthropic.claude-v2\"\n",
    "llm_model_id = \"anthropic.claude-instant-v1\"\n",
    "embed_model_id = \"cohere.embed-english-v3\"\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=b_client,\n",
    "    model_kwargs=model_kwargs,\n",
    "    model_id=llm_model_id,\n",
    "    region_name=region,\n",
    ")\n",
    "embeddings = BedrockEmbeddings(\n",
    "    client=b_client, model_id=embed_model_id, region_name=region\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc6cd3-9cf0-48ea-973e-88577c218b8b",
   "metadata": {},
   "source": [
    "### Scrape a few blogs posts for encoding\n",
    "\n",
    "We scrape top langchain blog posts from rss feed.\n",
    "\n",
    "We use LangChain `AsyncHtmlLoader` document loader to download blog posts as html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cae177-cb91-4798-9f36-e0e2be5e832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########################################################################################################################################| 15/15 [00:01<00:00, 14.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "feed_url = \"https://blog.langchain.dev/rss/\"\n",
    "rss_feed = feedparser.parse(feed_url)\n",
    "urls = [entry.link for entry in rss_feed.entries]\n",
    "\n",
    "html_loader = AsyncHtmlLoader(urls)\n",
    "html_docs = html_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d11958-cdcb-4c09-9335-80ab556de19b",
   "metadata": {},
   "source": [
    "Extracted html_docs metadata contains only `source`. Let's enhance the `metadata` by adding `language` and blog `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed36708c-9035-4fe7-bacf-39495de36469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for _html_doc in html_docs:\n",
    "    metadata = dict()\n",
    "    metadata[\"source\"] = _html_doc.metadata[\"source\"]\n",
    "    metadata[\"language\"] = \"en\"\n",
    "    soup = BeautifulSoup(_html_doc.page_content, \"html.parser\")\n",
    "    metadata[\"title\"] = soup.find(\"title\").text\n",
    "    _html_doc.metadata = metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f078a429-c78e-4c6e-90ef-bdf258ebc7ef",
   "metadata": {},
   "source": [
    "### Convert HTML docs into Text\n",
    "\n",
    "We use Unstructured [partition_html](https://unstructured-io.github.io/unstructured/core/partition.html#partition-html) to extract text from html. `partition_html` helps to clean and group html text.\n",
    "\n",
    "- group articles by title using `chunking_strategy='by_title'`\n",
    "- `assemble_articles = True`\n",
    "- `skip_headers_and_footers = True`\n",
    "- Clean any non ascii chars in text with `clean_non_ascii_chars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8044c08e-b7e5-4c22-860f-222e8eab5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from unstructured.cleaners.core import clean_non_ascii_chars\n",
    "from unstructured.partition.html import partition_html\n",
    "\n",
    "\n",
    "# Add documentation to the below function\n",
    "def extract_text_chunks_from_html(urls, html_docs) -> List[Document]:\n",
    "    \"\"\" \"\n",
    "    Function to reformat html_docs from html to plain text\n",
    "    Input: urls, html_docs\n",
    "    Output: List[Document]\n",
    "    \"\"\"\n",
    "    extracted_docs = []\n",
    "    for url, doc in zip(urls, html_docs):\n",
    "        elements = partition_html(\n",
    "            text=doc.page_content,\n",
    "            html_assemble_articles=True,\n",
    "            skip_headers_and_footers=True,\n",
    "            chunking_strategy=\"by_title\",\n",
    "        )\n",
    "        extracted_text = \"\".join([e.text for e in elements])\n",
    "        # extract links if available and append to metadata\n",
    "        extracted_links = []\n",
    "        for element in elements:\n",
    "            if element.metadata.links is not None:\n",
    "                print(element.metadata.links)\n",
    "                link = element.metadata.links[0][\"url\"][1:]\n",
    "                extracted_links.append(link)\n",
    "        # Add extracted links to metadata as references\n",
    "        if len(extracted_links) > 0:\n",
    "            doc.metadata[\"references\"] = extracted_links\n",
    "        doc.page_content = clean_non_ascii_chars(extracted_text)\n",
    "        extracted_docs.append(doc)\n",
    "    return extracted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4b3bd8-fbdc-4bb1-b3ff-0f44adc5d52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Converting <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> HTML docs to Text\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Converting \u001b[1;36m15\u001b[0m HTML docs to Text\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Converting {len(html_docs)} HTML docs to Text\")\n",
    "extracted_docs = extract_text_chunks_from_html(urls, html_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949dabf6-db38-45d4-a21c-e0b5f40e4c85",
   "metadata": {},
   "source": [
    "### Split docs into chunks the size of Embedding models max length (512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61acbbbc-3106-4811-b959-93107aa9fe33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Split <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> HTML docs into <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">108</span> chunks\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Split \u001b[1;36m15\u001b[0m HTML docs into \u001b[1;36m108\u001b[0m chunks\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Remember TextSplitter chunk_size is != model max length\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    add_start_index=True, chunk_size=2048, chunk_overlap=0\n",
    ")\n",
    "doc_chunks = splitter.split_documents(documents=extracted_docs)\n",
    "print(f\"Split {len(html_docs)} HTML docs into {len(doc_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f6417-a525-4433-bb25-e70bbe14b202",
   "metadata": {},
   "source": [
    "### Add docs to vectorstore (Qdrant)\n",
    "\n",
    "Install and run `qdrant` vector store locally using docker\n",
    "\n",
    "Refer here for Installation: <https://qdrant.tech/documentation/quick-start/>\n",
    "\n",
    "Qdrant should be running at port `6333` on localhost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9604b8-ad73-4f88-a514-ae0d747c3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs = False  # Set this to False during mutiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f37788b9-eb24-44c9-b731-fd739a5df56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Connected to collection: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">mlblogs_coherev3</span> ✅\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Connected to collection: \u001b[1;35mmlblogs_coherev3\u001b[0m ✅\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.vectorstores.qdrant import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "collection_name = \"mlblogs_coherev3\"  # define collection name\n",
    "qclient = QdrantClient(location=\"localhost\", port=6333)\n",
    "collection_status = qclient.get_collection(collection_name=collection_name).status\n",
    "\n",
    "if collection_status == \"green\":\n",
    "    print(f\"Connected to collection: [b magenta]{collection_name}[/b magenta] ✅\")\n",
    "    # Instantiating Qdrant client is weird with LangChain\n",
    "    db = Qdrant(\n",
    "        client=qclient,\n",
    "        collection_name=collection_name,\n",
    "        distance_strategy=\"cosine\",\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    \n",
    "if add_docs:\n",
    "    # Add documents to vector db with force_recreate = True for testing\n",
    "    db = db.from_documents(\n",
    "        documents=doc_chunks,\n",
    "        embedding=embeddings,\n",
    "        collection_name=collection_name,\n",
    "        force_recreate=False,  # Set this to false in PROD\n",
    "    )\n",
    "    print(\n",
    "        f\"Added [b]{len(doc_chunks)}[/b] to collection: [b green]{collection_name}[/b green] ✅\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03dc8fb-d091-4990-86ec-29a98a8fe5b8",
   "metadata": {},
   "source": [
    "### Enhance retrieval with MultiQueryRetriever (MQR)\n",
    "\n",
    "Idea here it to generate upto questions by rephrasing the original question provided by the user without changing the context of the question.\n",
    "\n",
    "We use our llm to rephrase our questions accordingly.\n",
    "\n",
    "`MultiQueryRetriever` does exactly that. Initialize MQR by passing in the retriever and llm.\n",
    "\n",
    "For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eccf0327-90dd-4f4d-ae7a-0b9e133eba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['Here are 3 alternative questions for \"What are some of the approaches to benchmark RAG on Tables\":', '', 'How can I evaluate the performance of RAG models on table data?', '', 'What methods exist for testing and comparing how well RAG models understand and interact with tabular information? ', '', 'What techniques or processes are used to measure how RAG algorithms handle and process information contained within tables?']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Extracted <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> queries\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Extracted \u001b[1;36m11\u001b[0m queries\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "# retriever_kwargs = {\"search_type\": \"similarity\", \"top_k\": 5}\n",
    "# qdrant_retriever = db.as_retriever(**retriever_kwargs)\n",
    "qdrant_retriever = db.as_retriever()\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=qdrant_retriever, llm=llm)\n",
    "\n",
    "question = \" What are some of the approaches to benchmark RAG on Tables\"\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "print(f\"Extracted {len(unique_docs)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927854c5-0f40-422e-85ba-900dc7aa3998",
   "metadata": {},
   "source": [
    "### Create MQR with your own prompt\n",
    "\n",
    "You can also supply a prompt along with an output parser to split the results into a list of queries.\n",
    "\n",
    "Output parser will split the LLM result into a list of queries by removing additional characters and cleaning empty strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cc7c87e-6655-4c72-b9ec-8039b1b2ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class QuestionsList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of rephrased questions\")\n",
    "\n",
    "\n",
    "class QuestionsListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=QuestionsList)\n",
    "\n",
    "    def parse(self, text: str) -> QuestionsList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        # remove 1. 2. from the string\n",
    "        questions = [re.sub(r\"^\\d+\\.\\s+\", \"\", question) for question in lines]\n",
    "        questions = [s for s in questions if s]  # remove any empty strings\n",
    "\n",
    "        return QuestionsList(lines=questions)\n",
    "\n",
    "\n",
    "output_parser = QuestionsListOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91495a2b-68e2-4d3f-b4d2-8802c3beb24f",
   "metadata": {},
   "source": [
    "#### Load and format question rephraser prompt from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30f22473-63ff-48f1-9f9e-f672f788ce96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "Human:Given the user's question provided in the <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">question</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt; tags, carefully analyze what information the user is </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">seeking.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Then generate </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> rephrased versions of the original question that ask for the same underlying information but </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">using different wording.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The goal of the rephrased questions is to retrieve additional relevant documents from the vector store to improve </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the chances of surfacing information to answer the user's need.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Make sure the rephrasings use synonymous vocabulary while retaining the intent of the original question.The </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">restatements should expand specificity where helpful to bring back useful context from the vector space.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Importantly, do NOT expand or assume meaning for any abbreviations used in the original question. Leave </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">abbreviations unchanged.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Do NOT output any preamble. Just output the rephrased questions.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Provide these rephrased questions separated by newlines.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">User's original question: &lt;question&gt; </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">question</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\"> &lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">question</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "Rephrased versions of the user's question:\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>.\n",
       "\n",
       "Assistant:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "Human:Given the user's question provided in the \u001b[1m<\u001b[0m\u001b[1;95mquestion\u001b[0m\u001b[39m> tags, carefully analyze what information the user is \u001b[0m\n",
       "\u001b[39mseeking.\u001b[0m\n",
       "\u001b[39mThen generate \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m rephrased versions of the original question that ask for the same underlying information but \u001b[0m\n",
       "\u001b[39musing different wording.\u001b[0m\n",
       "\n",
       "\u001b[39mThe goal of the rephrased questions is to retrieve additional relevant documents from the vector store to improve \u001b[0m\n",
       "\u001b[39mthe chances of surfacing information to answer the user's need.\u001b[0m\n",
       "\n",
       "\u001b[39mMake sure the rephrasings use synonymous vocabulary while retaining the intent of the original question.The \u001b[0m\n",
       "\u001b[39mrestatements should expand specificity where helpful to bring back useful context from the vector space.\u001b[0m\n",
       "\n",
       "\u001b[39mImportantly, do NOT expand or assume meaning for any abbreviations used in the original question. Leave \u001b[0m\n",
       "\u001b[39mabbreviations unchanged.\u001b[0m\n",
       "\n",
       "\u001b[39mDo NOT output any preamble. Just output the rephrased questions.\u001b[0m\n",
       "\n",
       "\u001b[39mProvide these rephrased questions separated by newlines.\u001b[0m\n",
       "\n",
       "\u001b[39mUser's original question: <question> \u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mquestion\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m <\u001b[0m\u001b[35m/\u001b[0m\u001b[95mquestion\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "Rephrased versions of the user's question:\n",
       "\u001b[1;36m1\u001b[0m.\n",
       "\u001b[1;36m2\u001b[0m.\n",
       "\u001b[1;36m3\u001b[0m.\n",
       "\n",
       "Assistant:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Let's load prompt from text file\n",
    "prompt_path = Path(\"./prompts/question_rephrase_claude.txt\")\n",
    "prompt_text = prompt_path.read_text(encoding=\"utf-8\")\n",
    "rephrase_prompt_text = f\"{HUMAN_PROMPT}{prompt_text}{AI_PROMPT}\"\n",
    "\n",
    "query_rephraser_prompt = PromptTemplate.from_template(template=rephrase_prompt_text)\n",
    "\n",
    "print(query_rephraser_prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f29ecf-d80e-4e00-87e1-d6a29d82ff03",
   "metadata": {},
   "source": [
    "### Build MQ retriever with question rephraser chain and custom output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "130865bc-6f77-442b-b0a4-4e72c4841135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What are some methods for evaluating RAG on tabular data?', 'What performance metrics can be used to assess RAG when applied to tables?  ', 'What techniques have researchers explored for measuring the effectiveness of RAG models on table understanding tasks?']\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# create question rephraser chain with llm\n",
    "rephraser_chain = LLMChain(llm=llm, prompt=query_rephraser_prompt, output_parser=output_parser)\n",
    "\n",
    "# build MQR\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=qdrant_retriever, llm_chain=rephraser_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "question = \" What are some of the approaches to benchmark RAG on Tables\"\n",
    "\n",
    "# Invoke MQR\n",
    "unique_docs = retriever.get_relevant_documents(query=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c4285-85c0-4a59-95bc-4fe6de5c5857",
   "metadata": {},
   "source": [
    "### Create RAG chain using enhanced MultiQuery retriever to answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c881f34-d94e-40cf-b0eb-f80af76dbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# qna chain with re-ranking\n",
    "rag_prompt_path = Path(\"./prompts/rag_prompt_claude.txt\")\n",
    "rag_prompt = PromptTemplate.from_file(rag_prompt_path, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# format retrieved docs within <context{idx}></context{idx}> tags\n",
    "def format_context_docs(docs):\n",
    "    context_string = \"\"\n",
    "    for idx, _d in enumerate(docs):\n",
    "        otag = f\"<context{idx+1}>\"\n",
    "        ctag = f\"</context{idx+1}>\"\n",
    "        c_text = f\"{otag} {_d.page_content} {ctag}\\n\"\n",
    "        context_string += c_text\n",
    "    return context_string\n",
    "\n",
    "\n",
    "# Input variables to RAG prompt are `context` and `question`\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_context_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        # \"context\": RunnableLambda(lambda output: format_context_docs(query=output, retriever=retriever)),\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc60715b-e061-4f4c-95e4-bb227265b078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Can we do Multimodal RAG on slide decks using langchain</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: \u001b[0m\u001b[1;32mCan we do Multimodal RAG on slide decks using langchain\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['Is it possible to perform Multimodal RAG on slide presentations using the langchain tool?', 'What are the capabilities for applying Multimodal RAG techniques to slideshow content such as PowerPoint decks through utilization of the langchain framework?  ', 'Can the langchain system facilitate Multimodal Relation-Aware Generation on slide deck files to analyze and generate content from the visual and textual elements?']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " <answer>Yes, LangChain supports multimodal RAG on slide decks using two main approaches: multi-modal embeddings and multi-vector retrieval. Multi-modal embeddings extract slides as images and use embeddings to retrieve relevant slides based on a user question, while multi-vector retrieval summarizes each slide image and embeds the summaries to retrieve relevant slides. A public benchmark was created to evaluate these approaches on a Datadog earnings presentation, finding multi-vector retrieval performed best.</answer>"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">=============================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "=============================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Difference between langchain core and community</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: \u001b[0m\u001b[1;32mDifference between langchain core and community\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What is the distinction between langchain core and community?', 'What are the key differences between langchain core versus community? ', 'How does langchain core diverge from or relate to the langchain community version?']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " <answer>\n",
       "Langchain-core contains simple, core abstractions that have emerged as a standard, as well as LangChain Expression Language as a way to compose these components together. Langchain-community contains all third party integrations.\n",
       "</answer>"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">=============================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "=============================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Are there any benchmarks for extraction</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: \u001b[0m\u001b[1;32mAre there any benchmarks for extraction\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What metrics exist for measuring extraction performance?', 'What standards are used to evaluate systems that extract information?  ', 'How can one assess the effectiveness of tools designed for extracting data?']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " <answer>Yes, the new dataset released offers a practical environment to test common challenges in LLM application development like classifying unstructured text, generating machine-readable information, and reasoning over multiple tasks with distracting information.</answer>"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">=============================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "=============================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">What are some of the approaches to benchmark RAG on Tables. Output in a bulleted list format.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: \u001b[0m\u001b[1;32mWhat are some of the approaches to benchmark RAG on Tables. Output in a bulleted list format.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What are some methods for evaluating the performance of RAG models on tabular data structures? Please provide responses in a bulleted list format.', 'How can RAG models be assessed when processing and understanding table-formatted information? Represent the answers as a bulleted listing of assessment techniques.  ', 'What testing or measurement procedures can be used to gauge how well RAG handles and comprehends table data? List the approaches without paragraphs in a bulleted format.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " <answer>\n",
       "- Text-to-SQL: Translating natural language into SQL requests to query tables. This enables evaluation on structured data while preserving data privacy.\n",
       "- Mixed type (structured and unstructured) data storage: Including an embedded document column using pgvector extension for PostgreSQL allows interacting with semi-structured data using natural language.\n",
       "</answer>"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">=============================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "=============================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "queries = [\n",
    "    \"Can we do Multimodal RAG on slide decks using langchain\",\n",
    "    \"Difference between langchain core and community\",\n",
    "    \"Are there any benchmarks for extraction\",\n",
    "    \"What are some of the approaches to benchmark RAG on Tables. Output in a bulleted list format.\"\n",
    "]\n",
    "\n",
    "# Invoke chain on all queries\n",
    "for q in queries:\n",
    "    print(f\"[b]Question: [b green]{q}[/b green]\")\n",
    "    output = rag_chain.invoke(q)\n",
    "    display(Markdown(output))\n",
    "    print(\"===\" * 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnthropicGH",
   "language": "python",
   "name": "anthropicgh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
