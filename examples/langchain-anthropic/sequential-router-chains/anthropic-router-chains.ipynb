{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb682b00-c862-4906-8cab-65f3684f94a8",
   "metadata": {},
   "source": [
    "# Langchain and Anthropic\n",
    "\n",
    "Claude is chat-based model, meaning it is trained on conversation data. However, it is a text based API, meaning it takes in single string. It expects this string to be in a particular format. This means that it is up the user to ensure that is the case. LangChain provides several utilities and helper functions to make sure prompts that you write - whether formatted as a string or as a list of messages - end up formatted correctly.\n",
    "\n",
    "Because Claude is chat-based but accepts a string as input, it can be treated as either a LangChain `ChatModel` or `LLM`. This means there are two wrappers in LangChain - ChatAnthropic and Anthropic. It is generally recommended to use the ChatAnthropic wrapper, and format your prompts as ChatMessages (we will show examples of this below). This is because it keeps your prompt in a general format that you can easily then also use with other models (should you want to). However, if you want more fine-grained control over the prompt, you can use the Anthropic wrapper - we will show and example of this as well. The `Anthropic` wrapper however is **deprecated**, as all functionality can be achieved in a more generic way using `ChatAnthropic`.\n",
    "\n",
    "Ref: <https://python.langchain.com/docs/integrations/platforms/anthropic>\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "#### No System Messages\n",
    "\n",
    "Anthropic models are not trained on the concept of a \"system message\". We have worked with the Anthropic team to handle them somewhat appropriately (a Human message with an admin tag) but this is largely a hack and it is recommended that you do not use system messages.\n",
    "\n",
    "#### AI Messages Can Continue\n",
    "\n",
    "A completion from Claude is a continuation of the last text in the string which allows you further control over Claude's output.\n",
    "\n",
    "For example, putting words in Claude's mouth in a prompt like this:\n",
    "\n",
    ">`\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: What do you call a bear with no teeth?`\n",
    "\n",
    "This will return a completion like this `A gummy bear!` instead of a whole new assistant message with a different random bear joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f1a3e1-c272-47b3-a2e9-27e3149bf2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uq boto3 anthropic-bedrock langchain rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db89590-e98b-4c40-9264-e0331229b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3f17cd-a8bc-4d7e-bcb7-96772de71bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from types import FunctionType\n",
    "\n",
    "import boto3\n",
    "from anthropic_bedrock import AI_PROMPT, HUMAN_PROMPT\n",
    "from IPython.display import Markdown\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "from rich import print\n",
    "from utils import get_inference_parameters\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cb69c-b2ce-4ca3-898f-f6507b0d2f2a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Initialize Bedrock LLM\n",
    "\n",
    "Here we utilize `langchain.llms.bedrock.Bedrock` class to initialize our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ebf95b4-ae41-4699-809b-d7a59bd737f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bedrock runtime client\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "bedrock_model_id = \"anthropic.claude-v2\"  # Bedrock model_id\n",
    "model_kwargs = get_inference_parameters(\"anthropic\")  # Model kwargs for Anthropic LLMs\n",
    "# print(model_kwargs)\n",
    "\n",
    "bedrock_model = Bedrock(\n",
    "    client=client, model_id=bedrock_model_id, model_kwargs=model_kwargs\n",
    ")  # Initalize LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6a43d-cf09-451c-b9b3-fc283b1cba8b",
   "metadata": {},
   "source": [
    "#### Creating Anthropic prompts with ChatPromptTemplate\n",
    "\n",
    "When working with ChatModels, it is preferred that you design your prompts as `ChatPromptTemplates`.\n",
    "\n",
    "Anthropic expects chat prompts to be formatted in a specific manner.\n",
    "\n",
    "```text\n",
    "\\n\\nHuman: Please translate the below text to french.\\n\\nAssistant:\n",
    "```\n",
    "\n",
    "We can chat messages by using the `human` and `assistant` prefixes when initializing the prompt. LangChain automatically adds the prefix and suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78989cc8-e061-4a16-8f4d-301fb4ac55a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'You are a helpful chatbot'</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Hi! How are you?'</span><span style=\"font-weight: bold\">))</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"I'm doing well. Thanks for asking.\"</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Tell me a joke about {topic}'</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mSystemMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m'You are a helpful chatbot'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m'Hi! How are you?'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m\"I\u001b[0m\u001b[32m'm doing well. Thanks for asking.\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m'Tell me a joke about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        (\"human\", \"Hi! How are you?\"),\n",
    "        (\"assistant\", \"I'm doing well. Thanks for asking.\"),\n",
    "        (\"human\", \"Tell me a joke about {topic}\"),\n",
    "    ]\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8907b4b-2bbd-42ff-8f3b-59532fa9670d",
   "metadata": {},
   "source": [
    "You can then invoke the chain like below:\n",
    "\n",
    "*NOTE:* The below example is using Langchain Expression Language (LCEL)\n",
    "\n",
    "Refer to <https://python.langchain.com/docs/expression_language> for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6898a356-10df-4018-b2a0-4fe8d5fa5349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\" Here's a silly joke about cats:\\n\\nWhy don't cats play poker in the jungle? Too many cheetahs!\"\u001b[0m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | bedrock_model\n",
    "chain.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0672f0b-5260-4cea-848c-976f47ede623",
   "metadata": {},
   "source": [
    "### Creating Prompts using `HumanMessage`\n",
    "\n",
    "Below we use `HumanMessage` from `langchain.schema` to construct `messages` for the chat model.\n",
    "\n",
    "We then create a `ChatPromptTemplate` from these messages to invoke this chain.\n",
    "\n",
    "\n",
    "*NOTE: As there are no input variables to be passed, we send an empty `input` string*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5047495b-d255-4c33-a551-08498e4c88c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Voici la traduction en français : J'adore programmer."
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Translate this sentence from English to French. I love programming.\"\n",
    "    )\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "\n",
    "chain = prompt | bedrock_model\n",
    "output = chain.invoke({\"input\": \"\"})\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c22cab1-812f-44f5-b55c-ac6fd614f5b0",
   "metadata": {},
   "source": [
    "##### Another Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce398b0-bea0-4cf1-a286-93d73007e4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Sorting algorithms are algorithms that put elements of a list in a certain order. Some common sorting algorithms include:\n",
       "\n",
       "- Bubble sort - Repeatedly steps through a list, compares adjacent elements, and swaps them if they are in the wrong order.\n",
       "\n",
       "- Insertion sort - Builds up a sorted list by inserting each new element into the correct position in the already sorted portion of the list.\n",
       "\n",
       "- Selection sort - Finds the smallest remaining element and adds it to the end of the sorted list.\n",
       "\n",
       "- Merge sort - Recursively divides a list into smaller sublists, sorts each sublist, then merges the sublists back together in sorted order. Uses a divide-and-conquer approach.\n",
       "\n",
       "- Quicksort - Chooses a pivot element and partitions the list into two sublists - elements less than the pivot and elements greater than the pivot. Recursively sorts the sublists.\n",
       "\n",
       "- Heapsort - Uses a binary heap data structure to sort the list. Builds a heap from the list, removes the largest element from the heap and places it at the end of the list, then reconstructs the heap with the remaining elements.\n",
       "\n",
       "The choice of sorting algorithm depends on things like efficiency, memory usage, and stability. Some algorithms like merge sort and heapsort are faster but require more memory. Others like insertion sort are simple to implement but slower. The optimal algorithm depends on the specific use case."
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"You're an AI assistant who's good at {ability}. Please answer this {question}\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "chain = prompt | bedrock_model\n",
    "\n",
    "output = chain.invoke(\n",
    "    {\"ability\": \"computerscience\", \"question\": \"What are sorting algorithms\"}\n",
    ")\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95ee05-e429-40ee-8c9c-aedba53b270e",
   "metadata": {},
   "source": [
    "#### Prompts with PromptTemplate\n",
    "\n",
    "We can see that under the hood LangChain is not appending any prefix/suffix to SystemMessage's. This is because `Anthropic` has no concept of `SystemMessage`. \n",
    "\n",
    "Anthropic requires all prompts to end with assistant messages. This means if the last message is not an assistant message, the suffix `Assistant:` will automatically be inserted.\n",
    "\n",
    "If you decide instead to use a normal `PromptTemplate` (one that just works on a single string) then we need do the following:\n",
    "\n",
    "- Prefix our prompt string with `HUMAN_PROMPT` and suffix with `AI_PROMPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd21f8af-b206-4485-a503-e554c000f528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n\\nHuman:Tell me a joke about {topic}\\n\\nAssistant:'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m'\\n\\nHuman:Tell me a joke about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\nAssistant:'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\" Here's a silly joke about otters:\\n\\nWhat do you call an otter with a cold? A snotter!\"\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "# Format prompt for Anthropic\n",
    "anthropic_prompt = PromptTemplate.from_template(\n",
    "    f\"{HUMAN_PROMPT}{prompt.template}{AI_PROMPT}\"\n",
    ")\n",
    "print(anthropic_prompt)\n",
    "\n",
    "# Invoke the Chain\n",
    "chain = anthropic_prompt | bedrock_model\n",
    "chain.invoke({\"topic\": \"otters\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79712a0-4428-484c-9f81-cbebc0a0d4f0",
   "metadata": {},
   "source": [
    "## Router Chain\n",
    "\n",
    "Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs.\n",
    "\n",
    "For e.g., Say we have four prompts optimized for different types of questions, and we want to choose the prompt template based on the user input.\n",
    "\n",
    "\n",
    "[!NOTE]: The below cells show a classical implementation of router chains. Implementing chains using LangChain Expression Language (LCEL) and Runnables is more intuitive and less verbose. Providing the below for comparision purposes.\n",
    "\n",
    "Refer to LCEL implementation of Router Chain below in this notebook.\n",
    "\n",
    "Reference: <https://python.langchain.com/docs/modules/chains/foundational/router>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36caddbd-255d-4033-b584-52ecb92e954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed5fedd0-991b-4bc2-b693-e65ff50dbc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\",\n",
    "        \"description\": \"Good for answering questions about physics\",\n",
    "        \"prompt_template\": physics_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\",\n",
    "        \"description\": \"Good for answering math questions\",\n",
    "        \"prompt_template\": math_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\",\n",
    "        \"description\": \"Good for answering history questions\",\n",
    "        \"prompt_template\": history_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\",\n",
    "        \"description\": \"Good for answering computer science questions\",\n",
    "        \"prompt_template\": computerscience_template,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc020af9-8b1b-4b83-be09-90a80b8a91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = bedrock_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a490a414-7635-49db-9789-0b3324e257fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7371c-7484-4f8d-916b-93ef1579de5c",
   "metadata": {},
   "source": [
    "#### Define a default_chain for inputs that doesn't match with any of the provided template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32016c81-8221-4bfc-8c69-d3ed8bf09f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5107a789-57f0-452b-96e9-664527233163",
   "metadata": {},
   "source": [
    "#### Define the Multi-prompt Router prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4599c4b1-4f91-4802-95b2-7d3039bc17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising \\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee2fc0fb-58da-482e-bda0-9483be72ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b6eca8-865f-4ce1-ad12-6c206b5fec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fd93870-9d90-45fb-aff3-56ae0f2d98ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "History: {'input': 'List last 5 presidents of USA and their term durations. Output in a Markdown table format.'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Here is a list of the last 5 presidents of the USA and their term durations in a Markdown table format:\n",
       "\n",
       "| President | Term |\n",
       "|-|-|  \n",
       "| Donald Trump | 2017 - 2021 |\n",
       "| Barack Obama | 2009 - 2017 |\n",
       "| George W. Bush | 2001 - 2009 |  \n",
       "| Bill Clinton | 1993 - 2001 |\n",
       "| George H.W. Bush | 1989 - 1993 |"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chain.run(\n",
    "    \"List last 5 presidents of USA and their term durations. Output in a Markdown table format.\"\n",
    ")\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88d13e-1945-48b5-95d0-a13bab93f45c",
   "metadata": {},
   "source": [
    "## Router Chains using LCEL\n",
    "\n",
    "There are two ways to perform routing:\n",
    "\n",
    "1. Using a RunnableBranch.\n",
    "2. Writing custom factory function that takes the input of a previous step and returns a **runnable**. Importantly, this should return a **runnable** and NOT actually execute.\n",
    "\n",
    "We'll illustrate both methods using a two step sequence where the first step classifies an input question as being about Math, Physics, Computer Science or History, then routes to a corresponding prompt chain. We also define a *General* chain as a fall back mechanism.\n",
    "\n",
    "Here, we create a simple `ROUTER_PROMPT_TEMPLATE` that classifies the input text into one of the above 4 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd11be9e-dc18-4aff-9030-582a5147c15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m' history'\u001b[0m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = bedrock_model\n",
    "\n",
    "ROUTER_PROMPT_TEMPLATE = \"\"\"Given the user question below, classify it as either being about `math`, `physics`, `computerscience` or `history`.\n",
    "                                     \n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    "\n",
    "classify_chain = (\n",
    "    PromptTemplate.from_template(template=ROUTER_PROMPT_TEMPLATE)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# test the chain\n",
    "classify_chain.invoke(\n",
    "    {\"question\": \"List last 5 presidents of USA and their term durations.\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7315b393-a545-4cd5-91c7-597e24dea2e3",
   "metadata": {},
   "source": [
    "#### Create 4 chains - physics, history, math and computerscience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e773b6-ee45-4d96-b5ed-adab44ef7d73",
   "metadata": {},
   "source": [
    "##### Physics chain to answer physics questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e7449b1-f3d6-49fb-9307-57badfa25558",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a <question> you admit\\\n",
    "that you don't know.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "physics_chain = PromptTemplate.from_template(template=physics_template) | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "037ba519-1fae-4c34-90a7-82eb57b32846",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "math_chain = PromptTemplate.from_template(template=math_template) | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51f0a2a6-193a-4305-b97a-21d924c04db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "history_chain = PromptTemplate.from_template(template=history_template) | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7d7f344-b6c0-40c7-9915-10f4ed70af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "cs_chain = PromptTemplate.from_template(template=computerscience_template) | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aea18972-914a-43a1-861f-ae3ba3e23f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General chain for fallback\n",
    "general_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Respond to the following question: <question>{question}</question> Answer:\"\"\"\n",
    "    )\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acecf709-a71f-4287-9021-e6897912892e",
   "metadata": {},
   "source": [
    "### Using a RunnableBranch\n",
    "\n",
    "A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\n",
    "\n",
    "If no provided conditions match, it runs the default runnable.\n",
    "\n",
    "Create a `RunnableBranch` that routes the input to the appropriate prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a97f020-edd1-4cfc-a227-fbb2a0c27317",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_branch = RunnableBranch(\n",
    "    (lambda x: \"math\" in x[\"topic\"].lower(), math_chain),\n",
    "    (lambda x: \"physics\" in x[\"topic\"].lower(), physics_chain),\n",
    "    (lambda x: \"history\" in x[\"topic\"].lower(), history_chain),\n",
    "    (lambda x: \"computerscience\" in x[\"topic\"].lower(), cs_chain),\n",
    "    general_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7088e2bc-96f9-4537-a94d-8502cedaed02",
   "metadata": {},
   "source": [
    "#### Putting it all together.\n",
    "\n",
    "First, we pass the input **question** to a chain that categorizes, the **question** into one of 4 categories:\n",
    "\n",
    "- Math\n",
    "- Physics\n",
    "- Computer Science\n",
    "- History\n",
    "\n",
    "Next, we pass the output to the `RunnableBranch`, which then, routes the **question** to the relevant *chain*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fb20547-d5d2-4d6c-be65-8f618daebb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = {\n",
    "    \"topic\": classify_chain,\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "} | prompt_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "097fbd4b-a63e-4066-b2f9-120e8717b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Input schema: \n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'RunnableParallelInput'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'object'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'properties'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Question'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'string'</span><span style=\"font-weight: bold\">}}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Input schema: \n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'title'\u001b[0m: \u001b[32m'RunnableParallelInput'\u001b[0m,\n",
       "    \u001b[32m'type'\u001b[0m: \u001b[32m'object'\u001b[0m,\n",
       "    \u001b[32m'properties'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'question'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Question'\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'string'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">========================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "========================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Output schema: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'RunnableBranchOutput'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Output schema: \n",
       "\u001b[1m{\u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'RunnableBranchOutput'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the input and output schema of the full_chain\n",
    "print(\"Input schema: \", full_chain.input_schema.schema())\n",
    "print(\"====\" * 10)\n",
    "print(\"Output schema: \", full_chain.output_schema.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6b62b02-8bc7-4284-a010-6aa397929c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question:</span> <span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">List last </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; font-style: italic\">5</span><span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\"> presidents of USA and their term durations. Output in a Markdown table format.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion:\u001b[0m \u001b[3;32mList last \u001b[0m\u001b[1;3;32m5\u001b[0m\u001b[3;32m presidents of USA and their term durations. Output in a Markdown table format.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Here is a table of the last 5 presidents of the USA and their term durations:\n",
       "\n",
       "| President | Term |\n",
       "|-|-|  \n",
       "| Joe Biden | 2021 - present |\n",
       "| Donald Trump | 2017 - 2021 |\n",
       "| Barack Obama | 2009 - 2017 |  \n",
       "| George W. Bush | 2001 - 2009 |\n",
       "| Bill Clinton | 1993 - 2001 |"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">---------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "---------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question:</span> <span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">What is </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; font-style: italic\">5</span><span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">% of </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; font-style: italic\">257</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion:\u001b[0m \u001b[3;32mWhat is \u001b[0m\u001b[1;3;32m5\u001b[0m\u001b[3;32m% of \u001b[0m\u001b[1;3;32m257\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Okay, let's break this down step-by-step:\n",
       "\n",
       "1) 5% means 5 out of 100\n",
       "2) To find 5% of a number, we divide the number by 100 and multiply by 5\n",
       "3) 257 / 100 = 2.57\n",
       "4) 2.57 * 5 = 12.85\n",
       "\n",
       "Therefore, 5% of 257 is 12.85."
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">---------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "---------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question:</span> <span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">What are black holes?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion:\u001b[0m \u001b[3;32mWhat are black holes?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Black holes are regions of space with gravitational fields so strong that nothing, including light, can escape from within the black hole's event horizon. Some key facts about black holes:\n",
       "\n",
       "- They form when massive stars collapse at the end of their life cycles. The gravitational collapse is so extreme that it causes a singularity, or a point of infinite density and space-time curvature. \n",
       "\n",
       "- They have an event horizon, which is the boundary beyond which nothing can escape the black hole's gravitational pull, not even light. The event horizon's size is proportional to the black hole's mass.\n",
       "\n",
       "- They continue to grow in mass as they absorb surrounding material like gas, dust and stars that get too close and cross the event horizon. \n",
       "\n",
       "- They do not actively \"suck\" material in; objects get caught in their gravity once they come within a certain distance.\n",
       "\n",
       "- They cannot be directly observed, but can be detected through their gravitational effects on nearby stars and gas. They also emit Hawking radiation.\n",
       "\n",
       "- Matter that falls into a black hole is thought to be compressed into a singularity, but the laws of physics as we know them break down inside the event horizon.\n",
       "\n",
       "- They are thought to exist at the center of most large galaxies, including our own Milky Way galaxy. The supermassive black hole at the Milky Way's center is called Sagittarius A*.\n",
       "\n",
       "In summary, black holes are extremely dense regions of spacetime from which nothing can escape once it crosses the event horizon. They fascinate scientists and appear to play major roles in the evolution of galaxies."
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">---------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "---------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question:</span> <span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">How to implement Interfaces in Python?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion:\u001b[0m \u001b[3;32mHow to implement Interfaces in Python?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Python does not have an interface construct like other object-oriented languages such as Java or C#. However, Python does allow for interfaces to be implemented through abstract base classes. Here is one way to implement interfaces in Python:\n",
       "\n",
       "1. Create an abstract base class and use the abc module to make it an official abstract class:\n",
       "\n",
       "```python\n",
       "from abc import ABC, abstractmethod\n",
       "\n",
       "class MyInterface(ABC):\n",
       "\n",
       "    @abstractmethod\n",
       "    def method1(self):\n",
       "        pass\n",
       "\n",
       "    @abstractmethod    \n",
       "    def method2(self):\n",
       "        pass\n",
       "```\n",
       "\n",
       "2. Any class that inherits from this abstract base class must implement all the abstract methods:\n",
       "\n",
       "```python \n",
       "class MyClass(MyInterface):\n",
       "\n",
       "    def method1(self):\n",
       "        print(\"Implementing method1\")\n",
       "\n",
       "    def method2(self):\n",
       "        print(\"Implementing method2\")\n",
       "```\n",
       "\n",
       "3. Attempting to instantiate the abstract base class directly will raise an error:\n",
       "\n",
       "```python\n",
       "i = MyInterface() # Raises TypeError\n",
       "```\n",
       "\n",
       "4. But instantiating a concrete subclass will work:\n",
       "\n",
       "```python\n",
       "c = MyClass() \n",
       "c.method1()\n",
       "c.method2()\n",
       "```\n",
       "\n",
       "So in summary, interfaces can be implemented by creating abstract base classes and having concrete classes inherit from them and implement the required methods. This allows for \"interface-like\" behavior in Python."
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">---------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "---------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions = [\n",
    "    \"List last 5 presidents of USA and their term durations. Output in a Markdown table format.\",\n",
    "    \"What is 5% of 257\",\n",
    "    \"What are black holes?\",\n",
    "    \"How to implement Interfaces in Python?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"[b]Question:[/b] [i green]{question}[/i green]\")\n",
    "    output = full_chain.invoke({\"question\": question})\n",
    "    display(Markdown(output))\n",
    "    print()\n",
    "    print(\"---\" * 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "nlp311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
