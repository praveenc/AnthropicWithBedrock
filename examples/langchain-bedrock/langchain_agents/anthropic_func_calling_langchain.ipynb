{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5b27ac-4417-4239-9e19-539aaf347463",
   "metadata": {},
   "source": [
    "# Anthropic function calling with Langchain Agents\n",
    "\n",
    "In this notebook we leveraage Langchain agents to perform function calling tasks. \n",
    "\n",
    "First we install the necessary python packages such as `duckduckgo-search`, `wikipedia`, and `xmltodict`, amongst others. \n",
    "\n",
    "- Initialize connection to **anthropic.claude-v2** using Bedrock\n",
    "- Define and test the following tools individually\n",
    "  - Wikipedia\n",
    "  - Web Search and\n",
    "  - PubMed search\n",
    "- Finally, we put this all together by initializing a Langchain agent executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d934971-c3ab-4aff-9c2b-79b5e2a46bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install duckduckgo-search wikipedia unstructured xmltodict --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0eb56f-3844-4e24-ad2f-eb85f62d68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ff7655-f260-4057-9c9f-fd251f2a2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "from anthropic_bedrock import AI_PROMPT, HUMAN_PROMPT, AnthropicBedrock\n",
    "from IPython.display import Markdown\n",
    "from langchain.agents import AgentType, Tool, initialize_agent, load_tools\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.tools import DuckDuckGoSearchRun, PubmedQueryRun, tool\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from rich import print\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b078e7fd-fab7-4385-b459-3fe33ebfb769",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-west-2\"\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\", region_name=region\n",
    ")  # bedrock runtime boto3 client\n",
    "bedrock_model_id = \"anthropic.claude-v2\"  # Amazon Bedrock Model ID for claude-v2\n",
    "\n",
    "# anthropic claude model parameters\n",
    "model_kwargs = {\n",
    "    \"max_tokens_to_sample\": 4096,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 0.999,\n",
    "    \"top_k\": 250,\n",
    "    \"stop_sequences\": [HUMAN_PROMPT],\n",
    "}\n",
    "\n",
    "# Define the llm with langchain.llms.bedrock\n",
    "bedrock_llm = Bedrock(\n",
    "    model_id=bedrock_model_id, client=bedrock_runtime, model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7cd42-a642-4d67-8383-397aa3cadf4e",
   "metadata": {},
   "source": [
    "## Let's define a few tools\n",
    "\n",
    "- [Wikipedia](https://wikipedia.org/) (Search wikipedia pages)\n",
    "- Web Search (Search web using [DuckDuckGo](https://duckduckgo.com/) search engine)\n",
    "- [PubMed](https://pubmed.ncbi.nlm.nih.gov/) (Medical papers and articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891751e3-9edc-4308-a749-6a1f3a2dcc8f",
   "metadata": {},
   "source": [
    "#### 01.Wikipedia Tool\n",
    "\n",
    "[Wikipedia](https://wikipedia.org/) is a multilingual free online encyclopedia written and maintained by a community of volunteers, through open collaboration. Wikipedia is the largest and most-read reference work in history. \n",
    "\n",
    ">*NOTE:* To use this tool, install `wikipedia` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "948adab2-65ba-48c6-948a-749357f7059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API wrapper for Wikipedia\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "\n",
    "# Define the Wikipedia tool with a description and the function to retrieve results from Wikipedia\n",
    "wikipedia_tool = Tool(\n",
    "    name=\"wikipedia\",\n",
    "    func=wikipedia.run,\n",
    "    description=\"Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\",\n",
    ")\n",
    "\n",
    "# Test the defined Wikipedia tool\n",
    "# Markdown(wikipedia_tool(\"What is the Archimedes Principle?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee94af3-3863-494e-86be-c9466aeeffb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 02.DuckDuckGo Search Tool\n",
    "\n",
    "[DuckDuckGo](https://duckduckgo.com/) is an internet privacy company most popularly known for their private search engine. The company emphasizes privacy and anonimity as one of the key principles behind all their products.\n",
    "\n",
    ">*NOTE:* To use this tool, install `duckduckgo-search` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4ab023-0fdc-4f68-bd02-6548e62c4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API wrapper for DuckDuckGo search\n",
    "duckduckgo_search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Define the DuckDuckGo tool using a description and the function to retrieve results from DuckDuckGo search\n",
    "duckduckgo_tool = Tool(\n",
    "    name=\"DuckDuckGoSearch\",\n",
    "    func=duckduckgo_search.run,\n",
    "    description=\"useful for when you need to answer questions about current events\",\n",
    ")\n",
    "\n",
    "# Test the DuckDuckGo tool\n",
    "# Markdown(duckduckgo_tool(\"Weather in Seattle, Washington?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1afa8-0148-40e3-b520-48f4895c9988",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 03.PubMed Tool\n",
    "\n",
    "[PubMed](https://pubmed.ncbi.nlm.nih.gov/) is a free search engine for medical papers and articles.\n",
    "\n",
    ">*NOTE:* To use this tool, install `xmltodict` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d8b874f-ed91-43b3-b539-0a3182cc0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API wrapper for PubMed search\n",
    "pubmed_search = PubmedQueryRun()\n",
    "\n",
    "# Define the PubMed tool using a description and the function to retrieve results from PubMed\n",
    "pubmed_tool = Tool(\n",
    "    name=\"PubmedQueryRun\",\n",
    "    func=pubmed_search.run,\n",
    "    description=\"Useful for when you need medical information\",\n",
    ")\n",
    "\n",
    "# Test PubMed tool\n",
    "# Markdown(pubmed_tool(\"Covid diagnosis\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18036a-665e-4b98-860c-667becfc3e8b",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "- Define all the tools as a list\n",
    "- Initialize the agent with tool list and `AgentType` of type `ZERO_SHOT_REACT_DESCRIPTION`\n",
    "- Explicity tell the agent to handle parsing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a2fff6-95d7-47db-b385-40bcd7329715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Here is my response:\n",
      "\n",
      "Thought: To answer a question about SageMaker serverless inference endpoints, Wikipedia likely does not have detailed technical information. DuckDuckGo may have some high-level overviews but not in-depth details. PubMed is not relevant. The best action is to explain what I know about SageMaker serverless inference endpoints.\n",
      "\n",
      "Final Answer: SageMaker serverless inference endpoints allow you to deploy machine learning models for real-time inference without having to manage servers. They automatically scale up and down to meet traffic demands with no idle capacity. You only pay for the compute time used for inferences. They are a fully managed, elastic inference service. Some key benefits are no server management, automatic scaling, sub-second latency, and pay-per-use pricing. They are useful for deploying production ML models and powering real-time prediction services.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'input'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about SageMaker serverless inference endpoints'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'output'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SageMaker serverless inference endpoints allow you to deploy machine learning models for real-time </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inference without having to manage servers. They automatically scale up and down to meet traffic demands with no </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">idle capacity. You only pay for the compute time used for inferences. They are a fully managed, elastic inference </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">service. Some key benefits are no server management, automatic scaling, sub-second latency, and pay-per-use </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pricing. They are useful for deploying production ML models and powering real-time prediction services.'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'input'\u001b[0m: \u001b[32m'Tell me about SageMaker serverless inference endpoints'\u001b[0m,\n",
       "    \u001b[32m'output'\u001b[0m: \u001b[32m'SageMaker serverless inference endpoints allow you to deploy machine learning models for real-time \u001b[0m\n",
       "\u001b[32minference without having to manage servers. They automatically scale up and down to meet traffic demands with no \u001b[0m\n",
       "\u001b[32midle capacity. You only pay for the compute time used for inferences. They are a fully managed, elastic inference \u001b[0m\n",
       "\u001b[32mservice. Some key benefits are no server management, automatic scaling, sub-second latency, and pay-per-use \u001b[0m\n",
       "\u001b[32mpricing. They are useful for deploying production ML models and powering real-time prediction services.'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a list of tools for the agent\n",
    "tools = [wikipedia_tool, duckduckgo_tool, pubmed_tool]\n",
    "\n",
    "# Initialize the agent with access to all the tooks in the list\n",
    "agent_executor = initialize_agent(\n",
    "    tools,\n",
    "    bedrock_llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "\n",
    "output_dict = agent_executor.invoke(\n",
    "    {\"input\": \"Tell me about SageMaker serverless inference endpoints\"}\n",
    ")\n",
    "print(output_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "nlp311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
