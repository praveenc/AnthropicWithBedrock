{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1ca1f7-5cb7-4623-b351-42b49a3f67a7",
   "metadata": {},
   "source": [
    "# Langchain and Anthropic\n",
    "\n",
    "Claude is chat-based model, meaning it is trained on conversation data. However, it is a text based API, meaning it takes in single string. It expects this string to be in a particular format. This means that it is up the user to ensure that is the case. LangChain provides several utilities and helper functions to make sure prompts that you write - whether formatted as a string or as a list of messages - end up formatted correctly.\n",
    "\n",
    "Because Claude is chat-based but accepts a string as input, it can be treated as either a LangChain `ChatModel` or `LLM`. This means there are two wrappers in LangChain - ChatAnthropic and Anthropic. It is generally recommended to use the ChatAnthropic wrapper, and format your prompts as ChatMessages (we will show examples of this below). This is because it keeps your prompt in a general format that you can easily then also use with other models (should you want to). However, if you want more fine-grained control over the prompt, you can use the Anthropic wrapper - we will show and example of this as well. The `Anthropic` wrapper however is **deprecated**, as all functionality can be achieved in a more generic way using `ChatAnthropic`.\n",
    "\n",
    "Ref: <https://python.langchain.com/docs/integrations/platforms/anthropic>\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "#### No System Messages\n",
    "\n",
    "Anthropic models are not trained on the concept of a \"system message\". We have worked with the Anthropic team to handle them somewhat appropriately (a Human message with an admin tag) but this is largely a hack and it is recommended that you do not use system messages.\n",
    "\n",
    "#### AI Messages Can Continue\n",
    "\n",
    "A completion from Claude is a continuation of the last text in the string which allows you further control over Claude's output.\n",
    "\n",
    "For example, putting words in Claude's mouth in a prompt like this:\n",
    "\n",
    ">`\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: What do you call a bear with no teeth?`\n",
    "\n",
    "This will return a completion like this `A gummy bear!` instead of a whole new assistant message with a different random bear joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e5285-dceb-4065-bd0a-57bdb6eab168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uq boto3 anthropic-bedrock langchain rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f1ffc-08b7-4501-95e8-b4b1d5f482b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43049c22-1a86-4947-a961-b3e63314dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from rich import print\n",
    "import boto3\n",
    "from anthropic_bedrock import HUMAN_PROMPT, AI_PROMPT\n",
    "from utils import get_inference_parameters\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33a72d-2eba-436f-82a6-965adccbaac8",
   "metadata": {},
   "source": [
    "### Initialize Bedrock LLM\n",
    "\n",
    "Here we utilize `langchain.llms.bedrock.Bedrock` class to initialize our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f3fd7-f11f-4475-b589-e7f9678074ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bedrock runtime client\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "bedrock_model_id = \"anthropic.claude-v2\"  # Bedrock model_id\n",
    "model_kwargs = get_inference_parameters(\"anthropic\")  # Model kwargs for Anthropic LLMs\n",
    "# print(model_kwargs)\n",
    "\n",
    "bedrock_model = Bedrock(\n",
    "    client=client, model_id=bedrock_model_id, model_kwargs=model_kwargs\n",
    ")  # Initalize LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4843bb-ae4e-4644-b5c0-6bf001ec85dc",
   "metadata": {},
   "source": [
    "#### Creating prompts with ChatPromptTemplate\n",
    "\n",
    "When working with ChatModels, it is preferred that you design your prompts as `ChatPromptTemplates`.\n",
    "\n",
    "Here is an example below of doing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0023ce7-4ab3-4bab-929a-72fcf5c547be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful chatbot\"),\n",
    "    (\"human\", \"Hi! How are you?\"),\n",
    "    (\"assistant\", \"I'm doing well. Thanks for asking.\"),\n",
    "    (\"human\", \"Tell me a joke about {topic}\"),\n",
    "])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a988d-7a42-40f2-bb3b-3dbea393485b",
   "metadata": {},
   "source": [
    "You can then invoke the chain like below:\n",
    "\n",
    "*NOTE:* The below example is using Langchain Expression Language (LCEL)\n",
    "\n",
    "Refer to <https://python.langchain.com/docs/expression_language> for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946030a-483a-4124-870e-c094e1320d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | bedrock_model\n",
    "chain.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51376500-67dd-4e8b-a404-ddaeb0ef2d67",
   "metadata": {},
   "source": [
    "### Creating Prompts with ChatPromptTemplate\n",
    "\n",
    "Below we use `HumanMessage` from `langchain.schema` to construct `messages` for the chat model.\n",
    "\n",
    "We then create a `ChatPromptTemplate` from these messages to invoke this chain.\n",
    "\n",
    "\n",
    "*NOTE: As there are no input variables to be passed, we send an empty `input` string*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367ba39-f5fc-43d9-852e-e94d5f7979bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Translate this sentence from English to French. I love programming.\"\n",
    "    )\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "\n",
    "chain = prompt | bedrock_model\n",
    "chain.invoke({\"input\": \"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60369f52-134b-40dd-83ad-4deb58882ba2",
   "metadata": {},
   "source": [
    "### Creating Prompts with ChatPromptTemplate\n",
    "\n",
    "We can see that under the hood LangChain is not appending any prefix/suffix to SystemMessage's. This is because `Anthropic` has no concept of `SystemMessage`. \n",
    "\n",
    "Anthropic requires all prompts to end with assistant messages. This means if the last message is not an assistant message, the suffix `Assistant:` will automatically be inserted.\n",
    "\n",
    "If you decide instead to use a normal `PromptTemplate` (one that just works on a single string) then we need do the following:\n",
    "\n",
    "- Prefix our prompt string with `HUMAN_PROMPT` and suffix with `AI_PROMPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b367b-8123-4eaa-ae93-cdcbe605bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic_bedrock import HUMAN_PROMPT, AI_PROMPT\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "# Format prompt for Anthropic\n",
    "anthropic_prompt = PromptTemplate.from_template(f\"{HUMAN_PROMPT}{prompt.template}{AI_PROMPT}\")\n",
    "print(anthropic_prompt)\n",
    "\n",
    "# Invoke the Chain\n",
    "chain = anthropic_prompt | bedrock_model\n",
    "chain.invoke({\"topic\": \"otters\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d80e38b-84fa-4028-a751-10a40d49ba9e",
   "metadata": {},
   "source": [
    "## Sequential Chain\n",
    "\n",
    "1. Simple Sequential Chain\n",
    "2. Complex Sequential Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6527f-d502-45d3-9551-136ff4c0ec64",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain\n",
    "\n",
    "**CHAIN_1** -> [*OUTPUT*] -> **CHAIN_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af56425-fd55-44cf-83c3-d3de3b2470a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic(anthropic_api_key=\"q2342a5-34534544\")\n",
    "\n",
    "llm = bedrock_model\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"What is the best name to describe a company that makes {product}? output just the best name you can think of. Only output one name and nothing else.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "s_chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "# print(s_chain_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425bb24c-a1da-45b8-a5fc-a01c3977fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 1\n",
    "second_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Write a 20 words description for the following company: {company_name}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "s_chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "# print(s_chain_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f36512-230f-4abe-8c6d-2bee63882d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[s_chain_one, s_chain_two], verbose=True\n",
    ")\n",
    "\n",
    "# print(overall_simple_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69cabd-7662-4da9-bce8-fcb5c4dcf498",
   "metadata": {},
   "source": [
    "#### Run simple sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b151eba-b335-4d57-b2cb-a9ec8211ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e7913-e6d3-4d96-882e-2bc6ae45b003",
   "metadata": {},
   "source": [
    "### Complex Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6875268-3057-4b15-bc90-96a2f00b161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c51c6-3016-4681-80ee-e65b33bb62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = \"La nourriture était médiocre. J'ai aimé le steak et la sauce qui l'accompagne. A part ça, tout le reste n'était que moi.\"\n",
    "input = \"Das Essen war mittelmäßig. Mir gefielen das Steak und die dazugehörige Soße. Ansonsten war alles andere nur mittelmäßig.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482576ee-7961-49bc-96d6-24d6d5ef938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Translate the following review to english:\" \"\\n\\n{review}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(llm=bedrock_model, prompt=first_prompt, output_key=\"eng_review\")\n",
    "\n",
    "# print(chain_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab98b02-11af-4502-886a-d969636f421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Can you summarize the following review in 1 sentence:\" \"\\n\\n{eng_review}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain 2: input= eng_review and output= summary\n",
    "chain_two = LLMChain(llm=bedrock_model, prompt=second_prompt, output_key=\"summary\")\n",
    "# print(chain_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39df40-7d3b-44dd-994c-fa616a4c1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"What language is the following review:\\n\\n{eng_review}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=bedrock_model, prompt=third_prompt, output_key=\"language\")\n",
    "# print(chain_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad789d-de92-4abd-bb1a-e9db4c595c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Write a follow up response to the following \"\n",
    "            \"summary in the specified language:\"\n",
    "            \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain 3: input= Review and output= language\n",
    "chain_four = LLMChain(\n",
    "    llm=bedrock_model, prompt=fourth_prompt, output_key=\"followup_message\"\n",
    ")\n",
    "# print(chain_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444c29e-b7f7-4c60-8f0d-c1cb595b3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"review\"],\n",
    "    output_variables=[\"eng_review\", \"summary\", \"followup_message\"],\n",
    "    verbose=True,\n",
    ")\n",
    "# print(overall_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2881b65-2f52-4587-bf23-753a2eb6556d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc74d65-e9fb-493e-9023-68b7c993e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c74a8-7f75-487b-97cb-0ded64ce17bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from anthropic_bedrock import HUMAN_PROMPT, AI_PROMPT\n",
    "# prompt_file = Path(\"./function_calling_prompt.txt\")\n",
    "# prompt = PromptTemplate.from_file(prompt_file, [\"tools_string\", \"user_input\"])\n",
    "\n",
    "# # Prefix prompt with HUMAN_PROMPT and suffix with AI_PROMPT\n",
    "# prompt.template = f\"{HUMAN_PROMPT}{prompt.template}{AI_PROMPT}\"\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30c97e-7dd9-4ff2-bf4a-1e0348d11190",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def output_parser(text):\n",
    "#     \"\"\"\n",
    "#     Function to extract text within <answer> tags\n",
    "\n",
    "#     input: text (str) - output from LLM\n",
    "\n",
    "#     output: extracted_text (str) - returns extracted text from <answer> tags if found.\n",
    "#     Else, returns \"no answer tags found\"\n",
    "\n",
    "#     \"\"\"\n",
    "#     match = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL)\n",
    "#     if match:\n",
    "#         extracted_text = match.group(1)\n",
    "#     else:\n",
    "#         extracted_text = \"no answer tags found\"\n",
    "#     return extracted_text\n",
    "\n",
    "# def extract_func_calls(llm_output: str):\n",
    "#     pattern = r\"<function_call>(.*?)</function_call>\"\n",
    "#     matches = re.findall(pattern, llm_output, re.DOTALL)\n",
    "#     return matches[0]\n",
    "#     # for match in matches:\n",
    "#     #     print(match)\n",
    "\n",
    "# # Prepare input variables\n",
    "# tools_string = add_tools()\n",
    "# question = \"What is the weather in Denver\"\n",
    "\n",
    "# chain_input = {\n",
    "#     \"tools_string\": tools_string,\n",
    "#     \"user_input\" : question\n",
    "# }\n",
    "\n",
    "# # create chain using LCEL\n",
    "# chain = prompt | bedrock_model | extract_func_calls | execute_function\n",
    "\n",
    "# # Invoke chain\n",
    "# print(f\"Invoking func calling chain to answer: [b]{question}[/b]\")\n",
    "\n",
    "# output = chain.invoke(chain_input)\n",
    "# print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "nlp311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
