{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdbce81f-9f5e-46cb-82ba-3708913c2186",
   "metadata": {},
   "source": [
    "# Langchain and Anthropic\n",
    "\n",
    "Claude is chat-based model, meaning it is trained on conversation data. However, it is a text based API, meaning it takes in single string. It expects this string to be in a particular format. This means that it is up the user to ensure that is the case. LangChain provides several utilities and helper functions to make sure prompts that you write - whether formatted as a string or as a list of messages - end up formatted correctly.\n",
    "\n",
    "Because Claude is chat-based but accepts a string as input, it can be treated as either a LangChain `ChatModel` or `LLM`. This means there are two wrappers in LangChain - ChatAnthropic and Anthropic. It is generally recommended to use the ChatAnthropic wrapper, and format your prompts as ChatMessages (we will show examples of this below). This is because it keeps your prompt in a general format that you can easily then also use with other models (should you want to). However, if you want more fine-grained control over the prompt, you can use the Anthropic wrapper - we will show and example of this as well. The `Anthropic` wrapper however is **deprecated**, as all functionality can be achieved in a more generic way using `ChatAnthropic`.\n",
    "\n",
    "Ref: <https://python.langchain.com/docs/integrations/platforms/anthropic>\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "#### No System Messages\n",
    "\n",
    "Anthropic models are not trained on the concept of a \"system message\". We have worked with the Anthropic team to handle them somewhat appropriately (a Human message with an admin tag) but this is largely a hack and it is recommended that you do not use system messages.\n",
    "\n",
    "#### AI Messages Can Continue\n",
    "\n",
    "A completion from Claude is a continuation of the last text in the string which allows you further control over Claude's output.\n",
    "\n",
    "For example, putting words in Claude's mouth in a prompt like this:\n",
    "\n",
    ">`\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: What do you call a bear with no teeth?`\n",
    "\n",
    "This will return a completion like this `A gummy bear!` instead of a whole new assistant message with a different random bear joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f1a3e1-c272-47b3-a2e9-27e3149bf2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uq boto3 anthropic-bedrock langchain rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4fff3c1-ac5f-4321-8c1f-428ea003e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + \"/utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3f17cd-a8bc-4d7e-bcb7-96772de71bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rich extension is already loaded. To reload it, use:\n",
      "  %reload_ext rich\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from types import FunctionType\n",
    "\n",
    "import boto3\n",
    "from anthropic_bedrock import AI_PROMPT, HUMAN_PROMPT\n",
    "from IPython.display import Markdown\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "from rich import print\n",
    "from utils import get_inference_parameters\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext rich\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750f982-7cc4-4c05-a0a3-1e843c1cb0ee",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Initialize Bedrock LLM\n",
    "\n",
    "Here we utilize `langchain.llms.bedrock.Bedrock` class to initialize our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ebf95b4-ae41-4699-809b-d7a59bd737f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-west-2\"\n",
    "\n",
    "# get bedrock runtime client\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "\n",
    "bedrock_model_id = \"anthropic.claude-v2\"  # Bedrock model_id\n",
    "model_kwargs = get_inference_parameters(\"anthropic\")  # Model kwargs for Anthropic LLMs\n",
    "# print(model_kwargs)\n",
    "\n",
    "bedrock_model = Bedrock(\n",
    "    client=client,\n",
    "    model_id=bedrock_model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    "    region_name=region,\n",
    ")  # Initalize LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc752117-f389-44a6-ac17-6e5fb81cf438",
   "metadata": {},
   "source": [
    "#### Creating Anthropic prompts with ChatPromptTemplate\n",
    "\n",
    "When working with ChatModels, it is preferred that you design your prompts as `ChatPromptTemplates`.\n",
    "\n",
    "Anthropic expects chat prompts to be formatted in a specific manner.\n",
    "\n",
    "```text\n",
    "\\n\\nHuman: Please translate the below text to french.\\n\\nAssistant:\n",
    "```\n",
    "\n",
    "We can chat messages by using the `human` and `assistant` prefixes when initializing the prompt. LangChain automatically adds the prefix and suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78989cc8-e061-4a16-8f4d-301fb4ac55a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'You are a helpful chatbot'</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Hi! How are you?'</span><span style=\"font-weight: bold\">))</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"I'm doing well. Thanks for asking.\"</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Tell me a joke about {topic}'</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mSystemMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m'You are a helpful chatbot'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m'Hi! How are you?'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m\"I\u001b[0m\u001b[32m'm doing well. Thanks for asking.\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m'Tell me a joke about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        (\"human\", \"Hi! How are you?\"),\n",
    "        (\"assistant\", \"I'm doing well. Thanks for asking.\"),\n",
    "        (\"human\", \"Tell me a joke about {topic}\"),\n",
    "    ]\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a276f14-35e2-4d94-8258-ccacee902eaa",
   "metadata": {},
   "source": [
    "You can then invoke the chain like below:\n",
    "\n",
    "*NOTE:* The below example is using Langchain Expression Language (LCEL)\n",
    "\n",
    "Refer to <https://python.langchain.com/docs/expression_language> for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6898a356-10df-4018-b2a0-4fe8d5fa5349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\" Here's a silly joke about cats:\\n\\nWhy don't cats play poker in the jungle? Too many cheetahs!\"\u001b[0m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | bedrock_model\n",
    "chain.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3a353-e847-4f08-916f-2a30552a53c3",
   "metadata": {},
   "source": [
    "### Creating Prompts using `HumanMessage`\n",
    "\n",
    "Below we use `HumanMessage` from `langchain.schema` to construct `messages` for the chat model.\n",
    "\n",
    "We then create a `ChatPromptTemplate` from these messages to invoke this chain.\n",
    "\n",
    "\n",
    "*NOTE: As there are no input variables to be passed, we send an empty `input` string*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5047495b-d255-4c33-a551-08498e4c88c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Voici la traduction en fran√ßais : J'adore programmer."
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Translate this sentence from English to French. I love programming.\"\n",
    "    )\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "\n",
    "chain = prompt | bedrock_model\n",
    "output = chain.invoke({\"input\": \"\"})\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631a742-77d5-48dd-9003-bacd238c1b4b",
   "metadata": {},
   "source": [
    "##### Another Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dce398b0-bea0-4cf1-a286-93d73007e4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Sorting algorithms are algorithms that put elements of a list in a certain order. Some common sorting algorithms include:\n",
       "\n",
       "- Bubble sort - Repeatedly steps through a list, compares adjacent elements, and swaps them if they are in the wrong order.\n",
       "\n",
       "- Insertion sort - Builds up a sorted list by inserting each new element into the correct position in the already sorted portion of the list.\n",
       "\n",
       "- Selection sort - Finds the smallest remaining element and adds it to the end of the sorted list.\n",
       "\n",
       "- Merge sort - Recursively divides a list into smaller sublists, sorts each sublist, then merges the sublists back together in sorted order. Uses a divide-and-conquer approach.\n",
       "\n",
       "- Quicksort - Chooses a pivot element and partitions the list into two sublists - elements less than the pivot and elements greater than the pivot. Recursively sorts the sublists.\n",
       "\n",
       "- Heapsort - Uses a binary heap data structure to sort the list. Builds a heap from the list, removes the largest element from the heap and places it at the end of the list, then reconstructs the heap with the remaining elements.\n",
       "\n",
       "The choice of sorting algorithm depends on things like efficiency, memory usage, stability, etc. Some algorithms like merge sort and heapsort are faster but require more memory. Others like insertion sort are simple to implement but slower. The optimal algorithm depends on the specific use case."
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"You're an AI assistant who's good at {ability}. Please answer this {question}\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "chain = prompt | bedrock_model\n",
    "\n",
    "output = chain.invoke(\n",
    "    {\"ability\": \"computerscience\", \"question\": \"What are sorting algorithms\"}\n",
    ")\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d527a5-bb56-4a3f-80cc-6a83564913e0",
   "metadata": {},
   "source": [
    "#### Prompts with PromptTemplate\n",
    "\n",
    "We can see that under the hood LangChain is not appending any prefix/suffix to SystemMessage's. This is because `Anthropic` has no concept of `SystemMessage`. \n",
    "\n",
    "Anthropic requires all prompts to end with assistant messages. This means if the last message is not an assistant message, the suffix `Assistant:` will automatically be inserted.\n",
    "\n",
    "If you decide instead to use a normal `PromptTemplate` (one that just works on a single string) then we need do the following:\n",
    "\n",
    "- Prefix our prompt string with `HUMAN_PROMPT` and suffix with `AI_PROMPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd21f8af-b206-4485-a503-e554c000f528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n\\nHuman:Tell me a joke about {topic}\\n\\nAssistant:'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m'\\n\\nHuman:Tell me a joke about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\nAssistant:'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\" Here's a silly joke about otters:\\n\\nWhat do you call an otter with a cold? A snotter!\"\u001b[0m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "# Format prompt for Anthropic\n",
    "anthropic_prompt = PromptTemplate.from_template(\n",
    "    f\"{HUMAN_PROMPT}{prompt.template}{AI_PROMPT}\"\n",
    ")\n",
    "print(anthropic_prompt)\n",
    "\n",
    "# Invoke the Chain\n",
    "chain = anthropic_prompt | bedrock_model\n",
    "chain.invoke({\"topic\": \"otters\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnthropicGH",
   "language": "python",
   "name": "anthropicgh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
