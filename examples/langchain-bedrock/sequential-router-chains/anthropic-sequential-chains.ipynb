{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1ca1f7-5cb7-4623-b351-42b49a3f67a7",
   "metadata": {},
   "source": [
    "# Langchain and Anthropic\n",
    "\n",
    "Claude is chat-based model, meaning it is trained on conversation data. However, it is a text based API, meaning it takes in single string. It expects this string to be in a particular format. This means that it is up the user to ensure that is the case. LangChain provides several utilities and helper functions to make sure prompts that you write - whether formatted as a string or as a list of messages - end up formatted correctly.\n",
    "\n",
    "Because Claude is chat-based but accepts a string as input, it can be treated as either a LangChain `ChatModel` or `LLM`. This means there are two wrappers in LangChain - ChatAnthropic and Anthropic. It is generally recommended to use the ChatAnthropic wrapper, and format your prompts as ChatMessages (we will show examples of this below). This is because it keeps your prompt in a general format that you can easily then also use with other models (should you want to). However, if you want more fine-grained control over the prompt, you can use the Anthropic wrapper - we will show and example of this as well. The `Anthropic` wrapper however is **deprecated**, as all functionality can be achieved in a more generic way using `ChatAnthropic`.\n",
    "\n",
    "Ref: <https://python.langchain.com/docs/integrations/platforms/anthropic>\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "#### No System Messages\n",
    "\n",
    "Anthropic models are not trained on the concept of a \"system message\". We have worked with the Anthropic team to handle them somewhat appropriately (a Human message with an admin tag) but this is largely a hack and it is recommended that you do not use system messages.\n",
    "\n",
    "#### AI Messages Can Continue\n",
    "\n",
    "A completion from Claude is a continuation of the last text in the string which allows you further control over Claude's output.\n",
    "\n",
    "For example, putting words in Claude's mouth in a prompt like this:\n",
    "\n",
    ">`\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: What do you call a bear with no teeth?`\n",
    "\n",
    "This will return a completion like this `A gummy bear!` instead of a whole new assistant message with a different random bear joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e5285-dceb-4065-bd0a-57bdb6eab168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uq boto3 anthropic-bedrock langchain rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "113f1ffc-08b7-4501-95e8-b4b1d5f482b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43049c22-1a86-4947-a961-b3e63314dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import boto3\n",
    "from anthropic_bedrock import AI_PROMPT, HUMAN_PROMPT\n",
    "from langchain.chains import SequentialChain, SimpleSequentialChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.schema import StrOutputParser, HumanMessage\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from rich import print\n",
    "from IPython.display import Markdown\n",
    "from utils import get_inference_parameters\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33a72d-2eba-436f-82a6-965adccbaac8",
   "metadata": {},
   "source": [
    "### Initialize Bedrock LLM\n",
    "\n",
    "Here we utilize `langchain.llms.bedrock.Bedrock` class to initialize our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333f3fd7-f11f-4475-b589-e7f9678074ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bedrock runtime client\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "bedrock_model_id = \"anthropic.claude-v2\"  # Bedrock model_id\n",
    "model_kwargs = get_inference_parameters(\"anthropic\")  # Model kwargs for Anthropic LLMs\n",
    "# print(model_kwargs)\n",
    "\n",
    "bedrock_model = Bedrock(\n",
    "    client=client, model_id=bedrock_model_id, model_kwargs=model_kwargs\n",
    ")  # Initalize LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4843bb-ae4e-4644-b5c0-6bf001ec85dc",
   "metadata": {},
   "source": [
    "#### Creating prompts with ChatPromptTemplate\n",
    "\n",
    "When working with ChatModels, it is preferred that you design your prompts as `ChatPromptTemplates`.\n",
    "\n",
    "Here is an example below of doing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0023ce7-4ab3-4bab-929a-72fcf5c547be",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        (\"human\", \"Hi! How are you?\"),\n",
    "        (\"assistant\", \"I'm doing well. Thanks for asking.\"),\n",
    "        (\"human\", \"Tell me a joke about {topic}\"),\n",
    "    ]\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a988d-7a42-40f2-bb3b-3dbea393485b",
   "metadata": {},
   "source": [
    "You can then invoke the chain like below:\n",
    "\n",
    "*NOTE:* The below example is using Langchain Expression Language (LCEL)\n",
    "\n",
    "Refer to <https://python.langchain.com/docs/expression_language> for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946030a-483a-4124-870e-c094e1320d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | bedrock_model\n",
    "chain.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51376500-67dd-4e8b-a404-ddaeb0ef2d67",
   "metadata": {},
   "source": [
    "### Creating Prompts with ChatPromptTemplate\n",
    "\n",
    "Below we use `HumanMessage` from `langchain.schema` to construct `messages` for the chat model.\n",
    "\n",
    "We then create a `ChatPromptTemplate` from these messages to invoke this chain.\n",
    "\n",
    "\n",
    "*NOTE: As there are no input variables to be passed, we send an empty `input` string*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367ba39-f5fc-43d9-852e-e94d5f7979bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Translate this sentence from English to French. I love programming.\"\n",
    "    )\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "\n",
    "chain = prompt | bedrock_model\n",
    "chain.invoke({\"input\": \"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60369f52-134b-40dd-83ad-4deb58882ba2",
   "metadata": {},
   "source": [
    "### Creating Prompts with ChatPromptTemplate\n",
    "\n",
    "We can see that under the hood LangChain is not appending any prefix/suffix to SystemMessage's. This is because `Anthropic` has no concept of `SystemMessage`. \n",
    "\n",
    "Anthropic requires all prompts to end with assistant messages. This means if the last message is not an assistant message, the suffix `Assistant:` will automatically be inserted.\n",
    "\n",
    "If you decide instead to use a normal `PromptTemplate` (one that just works on a single string) then we need do the following:\n",
    "\n",
    "- Prefix our prompt string with `HUMAN_PROMPT` and suffix with `AI_PROMPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b367b-8123-4eaa-ae93-cdcbe605bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "# Format prompt for Anthropic\n",
    "anthropic_prompt = PromptTemplate.from_template(\n",
    "    f\"{HUMAN_PROMPT}{prompt.template}{AI_PROMPT}\"\n",
    ")\n",
    "print(anthropic_prompt)\n",
    "\n",
    "# Invoke the Chain\n",
    "chain = anthropic_prompt | bedrock_model\n",
    "chain.invoke({\"topic\": \"otters\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a5cae4-90fd-4d1c-9b05-a77fa46c4f89",
   "metadata": {},
   "source": [
    "## Sequential calls to LLMs with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b41cfae4-39ef-4f9c-b3a7-076b6a780b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "llm = bedrock_model\n",
    "\n",
    "synopsis_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a playwright. Given the <title> of play, it is your job to write a synopsis for that title.\n",
    "\n",
    "<title>{title}</title>\n",
    "\n",
    "Playwright: This is a synopsis for the above play:\"\"\"\n",
    ")\n",
    "\n",
    "review_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a play critic from the New York Times. Given the <synopsis> of play, it is your job to write a review for that play.\n",
    "\n",
    "<synposis>{synopsis}</synposis>\n",
    "\n",
    "Review from a New York Times play critic of the above play:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349fe680-386c-486e-92fa-f25ececf5f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_chain = (\n",
    "    {\"synopsis\": synopsis_prompt | llm | StrOutputParser()}\n",
    "    | review_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c14522cb-0ac4-44cc-ad43-c3449f8a1ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Here is a potential New York Times review for the play \"Romance at Sunset on the Beach\":\n",
       "\n",
       "Romance Comes Crashing In at the Shore in 'Sunset on the Beach'\n",
       "\n",
       "The age-old tale of a chance seaside encounter leading to romance gets a modern treatment in the new play \"Romance at Sunset on the Beach,\" now showing at the Seaside Theater. While the plot may feel familiar, strong acting and clever dialogue lift this play above the typical meet-cute.\n",
       "\n",
       "Leading lady Sarah delivers an endearing performance as the introspective heroine enjoying a solitary walk on the beach. Her calm is delightfully disrupted when a literal run-in with a handsome stranger named James sends her belongings flying. The pair's immediate chemistry is evident from their witty banter, expertly delivered with comedic timing and flirtatious subtext. \n",
       "\n",
       "As the sun begins to set, painting the stage in dazzling hues, the action slows to a heartfelt pace. The audience is drawn in as Sarah and James bare their souls, admitting a sense of destiny in their unlikely meeting. We root for the pair as they share a tentative first kiss under the stars.\n",
       "\n",
       "While the plot borders on cliché, strong acting and sharp writing give this play an undeniable charm. The beauty of the staging during the sunset scene alone is worth the price of admission. Director Jane Smith clearly took great care to maximize the romanticism of the seaside setting.\n",
       "\n",
       "For audiences seeking an escape into a classic love story, \"Romance at Sunset on the Beach\" is a fine choice for an evening's entertainment. The tale may be familiar, but strong performances give it new life. Viewers are sure to leave the theater with a smile, reminiscing about their own chance encounters leading to romance."
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = sequential_chain.invoke({\"title\": \"Romance at sunset on the beach\"})\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec17e09c-15e7-4512-a0a2-8f2524ec9c12",
   "metadata": {},
   "source": [
    "## Multiple chains\n",
    "\n",
    "Runnables can easily be used to string together multiple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5e69608-f9f2-4fd6-a7f9-49246993bd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m' Barack Obama nació en Honolulu, Hawái y se crió allí. Más tarde se mudó a Chicago, Illinois de adulto. Honolulu está en Hawái, que es un estado de los Estados Unidos.'\u001b[0m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = bedrock_model\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c5cb07b-417a-4597-87ce-d198bacfdeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"generate a {attribute} color. Return the name of the color and nothing else:\"\n",
    ")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\"\n",
    ")\n",
    "prompt3 = ChatPromptTemplate.from_template(\n",
    "    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\"\n",
    ")\n",
    "prompt4 = ChatPromptTemplate.from_template(\n",
    "    \"What is the color of {fruit} and the flag of {country}?\"\n",
    ")\n",
    "\n",
    "model_parser = model | StrOutputParser()\n",
    "\n",
    "color_generator = (\n",
    "    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser}\n",
    ")\n",
    "color_to_fruit = prompt2 | model_parser\n",
    "color_to_country = prompt3 | model_parser\n",
    "question_generator = (\n",
    "    color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f55409e-6918-46ad-8fc1-b2a2327a00be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m' Orange is a reddish-yellow color. The flag of the Netherlands consists of three horizontal bands of red, white, and blue.'\u001b[0m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = question_generator.invoke(\"warm\")\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4470787-790c-4aa9-893a-3c4d7512d207",
   "metadata": {},
   "source": [
    "### Branching and Merging\n",
    "\n",
    "You may want the output of one component to be processed by 2 or more other components. RunnableMaps let you split or fork the chain so multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following:\n",
    "\n",
    "![Branching and Merging](images/langchain_complex_chain_text.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18ada7e9-d193-4852-833f-b6a1b983ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = (\n",
    "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "    | {\"base_response\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "arguments_for = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the pros or positive aspects of {base_response}\"\n",
    "    )\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "arguments_against = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the cons or negative aspects of {base_response}\"\n",
    "    )\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_responder = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"ai\", \"{original_response}\"),\n",
    "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
    "            (\"system\", \"Generate a final response given the critique\"),\n",
    "        ]\n",
    "    )\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    planner\n",
    "    | {\n",
    "        \"results_1\": arguments_for,\n",
    "        \"results_2\": arguments_against,\n",
    "        \"original_response\": itemgetter(\"base_response\"),\n",
    "    }\n",
    "    | final_responder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55ddb6c0-994d-4655-afb2-25a9527eba0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Here is a balanced response on the pros and cons of scrum:\n",
       "\n",
       "Scrum, like any framework or methodology, has its strengths and weaknesses. On the positive side, scrum can provide helpful structure, transparency, and continuous improvement when implemented well. The short sprints promote urgency and accountability, while rituals like retros foster collaboration and adaptation. Overall, scrum aims to empower teams to self-organize and deliver value quickly. \n",
       "\n",
       "However, the prescriptive nature of scrum is not ideal for every team or context. Time-boxed sprints can sometimes undermine quality and flexibility. Strict roles and an over-emphasis on metrics like velocity can be constraining. Daily standups often degrade in effectiveness over time. Distributed and newer teams may struggle with the ceremonies.\n",
       "\n",
       "Ultimately, scrum is just a tool. Teams should feel free to adapt it as needed rather than follow it dogmatically. The agile values of individuals over process, working software over documentation, customer collaboration and responding to change matter more than rituals or artifacts. Scrum works best when implemented in a flexible way, keeping the focus on continuous improvement and adding value, not blindly adhering to a prescribed formula. With pragmatism and common sense, most teams can get significant benefits from scrum while minimizing any weaknesses or limitations."
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chain.invoke({\"input\": \"scrum\"})\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d80e38b-84fa-4028-a751-10a40d49ba9e",
   "metadata": {},
   "source": [
    "## Sequential Chain [LEGACY]\n",
    "\n",
    "1. Simple Sequential Chain\n",
    "2. Complex Sequential Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6527f-d502-45d3-9551-136ff4c0ec64",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain\n",
    "\n",
    "**CHAIN_1** -> [*OUTPUT*] -> **CHAIN_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af56425-fd55-44cf-83c3-d3de3b2470a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatAnthropic(anthropic_api_key=\"q2342a5-34534544\")\n",
    "\n",
    "llm = bedrock_model\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"What is the best name to describe a company that makes {product}? output just the best name you can think of. Only output one name and nothing else.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "s_chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "# print(s_chain_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425bb24c-a1da-45b8-a5fc-a01c3977fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 1\n",
    "second_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Write a 20 words description for the following company: {company_name}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "s_chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "# print(s_chain_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f36512-230f-4abe-8c6d-2bee63882d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[s_chain_one, s_chain_two], verbose=True\n",
    ")\n",
    "\n",
    "# print(overall_simple_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69cabd-7662-4da9-bce8-fcb5c4dcf498",
   "metadata": {},
   "source": [
    "#### Run simple sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b151eba-b335-4d57-b2cb-a9ec8211ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e7913-e6d3-4d96-882e-2bc6ae45b003",
   "metadata": {},
   "source": [
    "### Complex Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c51c6-3016-4681-80ee-e65b33bb62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = \"La nourriture était médiocre. J'ai aimé le steak et la sauce qui l'accompagne. A part ça, tout le reste n'était que moi.\"\n",
    "input = \"Das Essen war mittelmäßig. Mir gefielen das Steak und die dazugehörige Soße. Ansonsten war alles andere nur mittelmäßig.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482576ee-7961-49bc-96d6-24d6d5ef938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Translate the following review to english:\" \"\\n\\n{review}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(llm=bedrock_model, prompt=first_prompt, output_key=\"eng_review\")\n",
    "\n",
    "# print(chain_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab98b02-11af-4502-886a-d969636f421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Can you summarize the following review in 1 sentence:\" \"\\n\\n{eng_review}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain 2: input= eng_review and output= summary\n",
    "chain_two = LLMChain(llm=bedrock_model, prompt=second_prompt, output_key=\"summary\")\n",
    "# print(chain_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39df40-7d3b-44dd-994c-fa616a4c1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"What language is the following review:\\n\\n{eng_review}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=bedrock_model, prompt=third_prompt, output_key=\"language\")\n",
    "# print(chain_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad789d-de92-4abd-bb1a-e9db4c595c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Write a follow up response to the following \"\n",
    "            \"summary in the specified language:\"\n",
    "            \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain 3: input= Review and output= language\n",
    "chain_four = LLMChain(\n",
    "    llm=bedrock_model, prompt=fourth_prompt, output_key=\"followup_message\"\n",
    ")\n",
    "# print(chain_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444c29e-b7f7-4c60-8f0d-c1cb595b3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"review\"],\n",
    "    output_variables=[\"eng_review\", \"summary\", \"followup_message\"],\n",
    "    verbose=True,\n",
    ")\n",
    "# print(overall_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2881b65-2f52-4587-bf23-753a2eb6556d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc74d65-e9fb-493e-9023-68b7c993e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "nlp311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
