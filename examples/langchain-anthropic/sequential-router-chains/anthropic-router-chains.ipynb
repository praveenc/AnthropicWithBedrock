{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb682b00-c862-4906-8cab-65f3684f94a8",
   "metadata": {},
   "source": [
    "# Langchain and Anthropic\n",
    "\n",
    "Claude is chat-based model, meaning it is trained on conversation data. However, it is a text based API, meaning it takes in single string. It expects this string to be in a particular format. This means that it is up the user to ensure that is the case. LangChain provides several utilities and helper functions to make sure prompts that you write - whether formatted as a string or as a list of messages - end up formatted correctly.\n",
    "\n",
    "Because Claude is chat-based but accepts a string as input, it can be treated as either a LangChain `ChatModel` or `LLM`. This means there are two wrappers in LangChain - ChatAnthropic and Anthropic. It is generally recommended to use the ChatAnthropic wrapper, and format your prompts as ChatMessages (we will show examples of this below). This is because it keeps your prompt in a general format that you can easily then also use with other models (should you want to). However, if you want more fine-grained control over the prompt, you can use the Anthropic wrapper - we will show and example of this as well. The `Anthropic` wrapper however is **deprecated**, as all functionality can be achieved in a more generic way using `ChatAnthropic`.\n",
    "\n",
    "Ref: <https://python.langchain.com/docs/integrations/platforms/anthropic>\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "#### No System Messages\n",
    "\n",
    "Anthropic models are not trained on the concept of a \"system message\". We have worked with the Anthropic team to handle them somewhat appropriately (a Human message with an admin tag) but this is largely a hack and it is recommended that you do not use system messages.\n",
    "\n",
    "#### AI Messages Can Continue\n",
    "\n",
    "A completion from Claude is a continuation of the last text in the string which allows you further control over Claude's output.\n",
    "\n",
    "For example, putting words in Claude's mouth in a prompt like this:\n",
    "\n",
    ">`\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: What do you call a bear with no teeth?`\n",
    "\n",
    "This will return a completion like this `A gummy bear!` instead of a whole new assistant message with a different random bear joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f1a3e1-c272-47b3-a2e9-27e3149bf2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uq boto3 anthropic-bedrock langchain rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db89590-e98b-4c40-9264-e0331229b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3f17cd-a8bc-4d7e-bcb7-96772de71bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from rich import print\n",
    "import boto3\n",
    "from anthropic_bedrock import HUMAN_PROMPT, AI_PROMPT\n",
    "from types import FunctionType\n",
    "from utils import get_inference_parameters\n",
    "import tools\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cb69c-b2ce-4ca3-898f-f6507b0d2f2a",
   "metadata": {},
   "source": [
    "### Initialize Bedrock LLM\n",
    "\n",
    "Here we utilize `langchain.llms.bedrock.Bedrock` class to initialize our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ebf95b4-ae41-4699-809b-d7a59bd737f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bedrock runtime client\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "bedrock_model_id = \"anthropic.claude-v2\"  # Bedrock model_id\n",
    "model_kwargs = get_inference_parameters(\"anthropic\")  # Model kwargs for Anthropic LLMs\n",
    "# print(model_kwargs)\n",
    "\n",
    "bedrock_model = Bedrock(\n",
    "    client=client, model_id=bedrock_model_id, model_kwargs=model_kwargs\n",
    ")  # Initalize LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6a43d-cf09-451c-b9b3-fc283b1cba8b",
   "metadata": {},
   "source": [
    "#### Creating prompts with ChatPromptTemplate\n",
    "\n",
    "When working with ChatModels, it is preferred that you design your prompts as `ChatPromptTemplates`.\n",
    "\n",
    "Here is an example below of doing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78989cc8-e061-4a16-8f4d-301fb4ac55a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'You are a helpful chatbot'</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Hi! How are you?'</span><span style=\"font-weight: bold\">))</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"I'm doing well. Thanks for asking.\"</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Tell me a joke about {topic}'</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mSystemMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m'You are a helpful chatbot'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m'Hi! How are you?'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m\"I\u001b[0m\u001b[32m'm doing well. Thanks for asking.\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m'Tell me a joke about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        (\"human\", \"Hi! How are you?\"),\n",
    "        (\"assistant\", \"I'm doing well. Thanks for asking.\"),\n",
    "        (\"human\", \"Tell me a joke about {topic}\"),\n",
    "    ]\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8907b4b-2bbd-42ff-8f3b-59532fa9670d",
   "metadata": {},
   "source": [
    "You can then invoke the chain like below:\n",
    "\n",
    "*NOTE:* The below example is using Langchain Expression Language (LCEL)\n",
    "\n",
    "Refer to <https://python.langchain.com/docs/expression_language> for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6898a356-10df-4018-b2a0-4fe8d5fa5349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\" Here's a silly joke about cats:\\n\\nWhy don't cats play poker in the jungle? Too many cheetahs!\"\u001b[0m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | bedrock_model\n",
    "chain.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0672f0b-5260-4cea-848c-976f47ede623",
   "metadata": {},
   "source": [
    "### Passing inputs using HumanMessage\n",
    "\n",
    "Below we use `HumanMessage` from `langchain.schema` to construct `messages` for the chat model.\n",
    "\n",
    "We then create a `ChatPromptTemplate` from these messages to invoke this chain.\n",
    "\n",
    "\n",
    "*NOTE: As there are no input variables to be passed, we send an empty `input` string*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5047495b-d255-4c33-a551-08498e4c88c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\" Voici la traduction en français : J'adore programmer.\"\u001b[0m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Translate this sentence from English to French. I love programming.\"\n",
    "    )\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "\n",
    "chain = prompt | bedrock_model\n",
    "chain.invoke({\"input\": \"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95ee05-e429-40ee-8c9c-aedba53b270e",
   "metadata": {},
   "source": [
    "#### Prompts with PromptTemplate\n",
    "\n",
    "We can see that under the hood LangChain is not appending any prefix/suffix to SystemMessage's. This is because `Anthropic` has no concept of `SystemMessage`. \n",
    "\n",
    "Anthropic requires all prompts to end with assistant messages. This means if the last message is not an assistant message, the suffix `Assistant:` will automatically be inserted.\n",
    "\n",
    "If you decide instead to use a normal `PromptTemplate` (one that just works on a single string) then we need do the following:\n",
    "\n",
    "- Prefix our prompt string with `HUMAN_PROMPT` and suffix with `AI_PROMPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd21f8af-b206-4485-a503-e554c000f528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n\\nHuman:Tell me a joke about {topic}\\n\\nAssistant:'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtemplate\u001b[0m=\u001b[32m'\\n\\nHuman:Tell me a joke about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\nAssistant:'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\" Here's a silly joke about otters:\\n\\nWhat do you call an otter with a cold? A snotter!\"\u001b[0m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from anthropic_bedrock import HUMAN_PROMPT, AI_PROMPT\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "# Format prompt for Anthropic\n",
    "anthropic_prompt = PromptTemplate.from_template(\n",
    "    f\"{HUMAN_PROMPT}{prompt.template}{AI_PROMPT}\"\n",
    ")\n",
    "print(anthropic_prompt)\n",
    "\n",
    "# Invoke the Chain\n",
    "chain = anthropic_prompt | bedrock_model\n",
    "chain.invoke({\"topic\": \"otters\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79712a0-4428-484c-9f81-cbebc0a0d4f0",
   "metadata": {},
   "source": [
    "## Router Chain\n",
    "\n",
    "Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs.\n",
    "\n",
    "As a very simple example, let's suppose we have four templates optimized for different types of questions, and we want to choose the template based on the user input.\n",
    "\n",
    "Reference: <https://python.langchain.com/docs/modules/chains/foundational/router>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36caddbd-255d-4033-b584-52ecb92e954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed5fedd0-991b-4bc2-b693-e65ff50dbc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\",\n",
    "        \"description\": \"Good for answering questions about physics\",\n",
    "        \"prompt_template\": physics_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\",\n",
    "        \"description\": \"Good for answering math questions\",\n",
    "        \"prompt_template\": math_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\",\n",
    "        \"description\": \"Good for answering history questions\",\n",
    "        \"prompt_template\": history_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\",\n",
    "        \"description\": \"Good for answering computer science questions\",\n",
    "        \"prompt_template\": computerscience_template,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc2db3ac-49d2-4df9-8a2f-08bc8b04000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc020af9-8b1b-4b83-be09-90a80b8a91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = bedrock_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a490a414-7635-49db-9789-0b3324e257fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9f9d6ac-c359-4979-8ead-009c158fd6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'physics'\u001b[0m: \u001b[1;35mLLMChain\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mprompt\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'input'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'input'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                        \u001b[33mtemplate\u001b[0m=\u001b[32m\"You\u001b[0m\u001b[32m are a very smart physics professor. You are great at answering questions about physics in a conciseand easy to understand manner. When you don't know the answer to a question you admitthat you don't know.\\n\\nHere is a question:\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32minput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mllm\u001b[0m=\u001b[1;35mBedrock\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mclient\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mbotocore.client.BedrockRuntime\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x137d28fd0\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mmodel_id\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'anthropic.claude-v2'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mmodel_kwargs\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'max_tokens_to_sample'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m1000\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'temperature'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'top_k'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m250\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'top_p'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'stop_sequences'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'\\n\\nHuman:'\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'math'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mLLMChain\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mprompt\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33minput_variables\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mmessages\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[33mprompt\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mPromptTemplate\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33minput_variables\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mtemplate\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts, \\nanswer the component parts, and then put them togetherto answer the broader question.\\n\\nHere is a question:\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32minput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mllm\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mBedrock\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mclient\u001b[0m\u001b[39m=<botocore.client.BedrockRuntime object at \u001b[0m\u001b[1;36m0x137d28fd0\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mmodel_id\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'anthropic.claude-v2'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mmodel_kwargs\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'max_tokens_to_sample'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m1000\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'temperature'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'top_k'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m250\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'top_p'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'stop_sequences'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'\\n\\nHuman:'\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'History'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mLLMChain\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mprompt\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33minput_variables\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mmessages\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[33mprompt\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mPromptTemplate\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33minput_variables\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mtemplate\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'You are a very good historian. You have an excellent knowledge of and understanding of people,events and contexts from a range of historical periods. You have the ability to think, reflect, debate, discuss and evaluate the past. You have a respect for historical evidenceand the ability to make use of it to support your explanations and judgements.\\n\\nHere is a question:\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32minput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mllm\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mBedrock\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mclient\u001b[0m\u001b[39m=<botocore.client.BedrockRuntime object at \u001b[0m\u001b[1;36m0x137d28fd0\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mmodel_id\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'anthropic.claude-v2'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mmodel_kwargs\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'max_tokens_to_sample'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m1000\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'temperature'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'top_k'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m250\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'top_p'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[32m'stop_sequences'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'\\n\\nHuman:'\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'computer science'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mLLMChain\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mprompt\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33minput_variables\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mmessages\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[33mprompt\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mPromptTemplate\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33minput_variables\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mtemplate\u001b[0m\u001b[39m=\u001b[0m\u001b[32m' You are a successful computer scientist.You have a passion for creativity, collaboration,forward-thinking, confidence, strong problem-solving capabilities,understanding of theories and algorithms, and excellent communication skills. You are great at answering coding questions. You are so good because you know how to solve a problem by describing the solution in imperative steps that a machine can easily interpret and you know how to choose a solution that has a good balance between time complexity and space complexity. \\n\\nHere is a question:\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32minput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mllm\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mBedrock\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mclient\u001b[0m\u001b[39m=<botocore.client.BedrockRuntime object at \u001b[0m\u001b[1;36m0x137d28fd0\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[33mmodel_id\u001b[0m=\u001b[32m'anthropic.claude-v2'\u001b[0m,\n",
       "            \u001b[33mmodel_kwargs\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'max_tokens_to_sample'\u001b[0m: \u001b[1;36m1000\u001b[0m,\n",
       "                \u001b[32m'temperature'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "                \u001b[32m'top_k'\u001b[0m: \u001b[1;36m250\u001b[0m,\n",
       "                \u001b[32m'top_p'\u001b[0m: \u001b[1;36m0.999\u001b[0m,\n",
       "                \u001b[32m'stop_sequences'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'\\n\\nHuman:'\u001b[0m\u001b[1m]\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# destination_chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7371c-7484-4f8d-916b-93ef1579de5c",
   "metadata": {},
   "source": [
    "#### Define a default_chain for inputs that doesn't match with any of the provided template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32016c81-8221-4bfc-8c69-d3ed8bf09f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5107a789-57f0-452b-96e9-664527233163",
   "metadata": {},
   "source": [
    "#### Define the Multi-prompt Router prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4599c4b1-4f91-4802-95b2-7d3039bc17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising \\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee2fc0fb-58da-482e-bda0-9483be72ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81b6eca8-865f-4ce1-ad12-6c206b5fec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fd93870-9d90-45fb-aff3-56ae0f2d98ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "History: {'input': 'List last 5 presidents of USA and their term durations. Output in a Markdown table format.'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Here is a list of the last 5 presidents of the USA and their term durations in Markdown table format:\n",
       "\n",
       "| President | Term |\n",
       "|-|-|  \n",
       "| Donald Trump | 2017 - 2021 |\n",
       "| Barack Obama | 2009 - 2017 |\n",
       "| George W. Bush | 2001 - 2009 | \n",
       "| Bill Clinton | 1993 - 2001 |\n",
       "| George H.W. Bush | 1989 - 1993 |"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "output = chain.run(\n",
    "    \"List last 5 presidents of USA and their term durations. Output in a Markdown table format.\"\n",
    ")\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88d13e-1945-48b5-95d0-a13bab93f45c",
   "metadata": {},
   "source": [
    "## TODO: Router Chains using LCEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "nlp311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
